{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of RandomForest with SMOTE vs Augmented Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this blog I'd like to show the difference deep tabular augmentation can have when training a Random Forest on a highly biased data base. In this case, we have a look at credit card fraud, where fraud itself is is way less represented than non-fraud. I want to compare the popular SMOTE technique with the Deep Learning Augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from functools import partial\n",
    "import mlprepare as mlp\n",
    "import deep_tabular_augmentation as dta\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "DATA_PATH = 'data/creditcard.csv'\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a short look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's have a look of how many more non-fraud cases we have compared to fraud cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283823"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference_in_class_occurences = df['Class'].value_counts()[0]-df['Class'].value_counts()[1]\n",
    "difference_in_class_occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make use of the deep tabular augmentation we need to scale the data and then use only those cases, in which class we are interested in, in this case \"Class\" is equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = mlp.split_df(df, dep_var='Class', test_size=0.3, split_mode='random')\n",
    "\n",
    "x_scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = x_scaler.fit_transform(X_train)\n",
    "\n",
    "X_test_scaled = x_scaler.transform(X_test)\n",
    "\n",
    "X_train_fraud = X_train_scaled[np.where(y_train==1)[0]]\n",
    "X_test_fraud = X_test_scaled[np.where(y_test==1)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our model to work we need to put our data in a DataLoader (here I use the DataBunch Class from deep data augmentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = dta.create_datasets(X_train_fraud, y_train.values[np.where(y_train==1)], X_test_fraud, y_test.values[np.where(y_test==1)])\n",
    "data = dta.DataBunch(*dta.create_loaders(datasets, bs=1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're already good to go. We can define our Variational Encoder Architecture (here: 50->12->12->5->12->12->50) and then use the LearningRate Finder to tell us the best Learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in = X_train_fraud.shape[1]\n",
    "VAE_arch = [50, 12, 12]\n",
    "target_name = 'Class'\n",
    "target_class = 1\n",
    "df_cols = list(df.columns)\n",
    "\n",
    "model = dta.Autoencoder(D_in, VAE_arch, latent_dim=5).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_func = dta.customLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = dta.Learner(model, opt, loss_func, data, target_name, target_class, df_cols)\n",
    "\n",
    "run = dta.Runner(cb_funcs=[dta.LR_Find, dta.Recorder])\n",
    "\n",
    "run.fit(100, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.recorder.plot(skip_last=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up a desirable learning rate and scheduler for our learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sched = dta.combine_scheds([0.3, 0.7], [dta.sched_cos(0.01, 0.1), dta.sched_cos(0.1, 0.01)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50\n",
      "train loss is: 235061.25\n",
      "validation loss is: 106866.8203125\n",
      "epoch: 100\n",
      "train loss is: 170239.125\n",
      "validation loss is: 75203.1640625\n",
      "epoch: 150\n",
      "train loss is: 120978.515625\n",
      "validation loss is: 149698.640625\n",
      "epoch: 200\n",
      "train loss is: 87892.9296875\n",
      "validation loss is: 129409.578125\n",
      "epoch: 250\n",
      "train loss is: 69032.921875\n",
      "validation loss is: 100179.2109375\n",
      "epoch: 300\n",
      "train loss is: 57270.7421875\n",
      "validation loss is: 81863.3671875\n",
      "epoch: 350\n",
      "train loss is: 49296.77734375\n",
      "validation loss is: 69471.0546875\n",
      "epoch: 400\n",
      "train loss is: 43532.91015625\n",
      "validation loss is: 60559.10546875\n"
     ]
    }
   ],
   "source": [
    "cbfs = [partial(dta.LossTracker, show_every=50), dta.Recorder, partial(dta.ParamScheduler, 'lr', sched)]\n",
    "model = dta.Autoencoder(D_in, VAE_arch, latent_dim=20).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "learn = dta.Learner(model, opt, loss_func, data, target_name, target_class, df_cols)\n",
    "run = dta.Runner(cb_funcs=cbfs)\n",
    "run.fit(400, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the created data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 25% of real standard deviation\n",
    "sigma = list(df[df['Class']==1][df_cols].std()*0.25)\n",
    "df_fake_with_noise = run.predict_with_noise_df(learn, no_samples=1000, mu=0, sigma=sigma, scaler=x_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>92906.046144</td>\n",
       "      <td>-2.386666</td>\n",
       "      <td>1.423976</td>\n",
       "      <td>-4.081175</td>\n",
       "      <td>2.416863</td>\n",
       "      <td>-1.921564</td>\n",
       "      <td>-1.03062</td>\n",
       "      <td>-3.456801</td>\n",
       "      <td>1.271546</td>\n",
       "      <td>-1.98204</td>\n",
       "      <td>...</td>\n",
       "      <td>1.012225</td>\n",
       "      <td>0.446317</td>\n",
       "      <td>0.208623</td>\n",
       "      <td>-0.239074</td>\n",
       "      <td>-0.014651</td>\n",
       "      <td>0.049692</td>\n",
       "      <td>0.773771</td>\n",
       "      <td>-0.056647</td>\n",
       "      <td>131.269742</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Time        V1        V2        V3        V4        V5       V6  \\\n",
       "mean  92906.046144 -2.386666  1.423976 -4.081175  2.416863 -1.921564 -1.03062   \n",
       "\n",
       "            V7        V8       V9  ...       V21       V22       V23  \\\n",
       "mean -3.456801  1.271546 -1.98204  ...  1.012225  0.446317  0.208623   \n",
       "\n",
       "           V24       V25       V26       V27       V28      Amount  Class  \n",
       "mean -0.239074 -0.014651  0.049692  0.773771 -0.056647  131.269742    1.0  \n",
       "\n",
       "[1 rows x 31 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fake_with_noise.describe().loc[['mean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use SMOTE for creating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    199032\n",
       "1       332\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    199032\n",
       "1    199032\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_res.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically SMOTE creates as many entries of the minority class until we have the same amount of cases in both classes. We can do this with the DeepLearning Augmenter as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198700, 31)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference_in_trainset = y_train.value_counts()[0]-y_train.value_counts()[1]\n",
    "df_fake_with_noise = run.predict_with_noise_df(learn, no_samples=difference_in_trainset, mu=0, sigma=sigma, scaler=x_scaler)\n",
    "df_fake_with_noise.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While SMOTE creates these synthetic data in almost no-time, the DeepLearning Augmentation takes about a minute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compare how the built-in class_weight functionality performs vs the new approach vs SMOTE. So we will build three trainsets: the original one, the one with additional data from SMOTE, and the one with additional data from DeepLearning Augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "train_df_fake_with_noise = pd.concat([train_df, df_fake_with_noise])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things easier to understand, let's define the datasets on which to train and on which to assess the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_train_aug = train_df.iloc[:,:30].values, test_df.iloc[:,:30].values, train_df_fake_with_noise.iloc[:,:30].values\n",
    "y_train, y_test, y_train_aug = train_df.iloc[:,30].values, test_df.iloc[:,30].values, train_df_fake_with_noise.iloc[:,30].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((199364, 30), (398064, 30), (398064, 30))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_train_aug.shape, X_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((199364,), (398064,), (398064,))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_train_aug.shape, y_res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's train model on the original data while using the differences in class occurences as weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf(xs, y, n_estimators=40, max_samples=500,\n",
    "       max_features=0.5, min_samples_leaf=5, **kwargs):\n",
    "    return RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators,\n",
    "        max_samples=max_samples, max_features=max_features,\n",
    "        min_samples_leaf=min_samples_leaf, oob_score=True, class_weight={0:1,1:difference_in_class_occurences}).fit(xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[85304,     3],\n",
       "       [  132,     4]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = rf(X_train, y_train)\n",
    "confusion_matrix(y_test, np.round(m.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the SMOTE data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_aug(xs, y, n_estimators=40, max_samples=500,\n",
    "       max_features=0.5, min_samples_leaf=5, **kwargs):\n",
    "    return RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators,\n",
    "        max_samples=max_samples, max_features=max_features,\n",
    "        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[84024,  1283],\n",
       "       [   11,   125]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_smote = rf_aug(X_res.values, y_res)\n",
    "confusion_matrix(y_test, np.round(m_smote.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the augmented dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[85249,    58],\n",
       "       [   29,   107]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_aug = rf_aug(X_train_aug, y_train_aug)\n",
    "confusion_matrix(y_test, np.round(m_aug.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the Classification Reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    no-fraud       1.00      1.00      1.00     85307\n",
      "       fraud       0.57      0.03      0.06       136\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.78      0.51      0.53     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['no-fraud', 'fraud']\n",
    "print(classification_report(y_test, np.round(m.predict(X_test)), target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    no-fraud       1.00      0.98      0.99     85307\n",
      "       fraud       0.09      0.92      0.16       136\n",
      "\n",
      "    accuracy                           0.98     85443\n",
      "   macro avg       0.54      0.95      0.58     85443\n",
      "weighted avg       1.00      0.98      0.99     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, np.round(m_smote.predict(X_test)), target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    no-fraud       1.00      1.00      1.00     85307\n",
      "       fraud       0.65      0.79      0.71       136\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.82      0.89      0.86     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, np.round(m_aug.predict(X_test)), target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see quite huge differences between the three approaches. Simply attaching a higher weight to the fraud-class didn't help at all, we were only able to identify 4 fraud cases correctly. The SMOTE approach lead to way more identified fraud cases, it found 125 cases out of 136. This leads to an recall of 0.92. However, this comes at a cost: we have astonishing 1283 missclassified fraud cases, leading to a precision in the fraud case of 0.09. The Deep Learning Augmentation correctly predicted 107 out of the 136 cases while only misclassifying 58 cases - this leads to a precision of 0.65 in the fraud case. \n",
    "\n",
    "To conclude, I think this blog was able to show the merits of Deep Learning Augmentation. While increasing the correctly identified fraud cases we were also able to keep a high precision in this case, meaning we only have a few misclassified cases. While SMOTE was able to correctly identify a few more fraud cases it did also create a huge amount of misclassified fraud cases which might be costly when it comes to resource allocation.\n",
    "\n",
    "Lasse"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
