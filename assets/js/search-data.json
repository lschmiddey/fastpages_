{
  
    
        "post0": {
            "title": "Data Augmentation for tabular data",
            "content": "How to create fake tabular data with a variational autoencoder to improve deep learning algorithms . To train deeplearning models the more data the better. When we&#39;re thinking of image data, the deeplearnig community thought about a lot of tricks how to enhance the model given a dataset of images. Meaning that by rotating, flipping, blurring etc. the image we can create more input data and also improve our model. . However, when thinking about tabular data, only few of these techniques exist. In this blogpost I want to show you how to create a variational autoencoder and make use of data augmentation. I will create fake data, which is sampled from the learned distribution of the underlying data. . import torch import torch.nn as nn import torch.nn.functional as F from torch import nn, optim from torch.autograd import Variable from sklearn.decomposition import PCA import pandas as pd import numpy as np from sklearn import preprocessing from sklearn.model_selection import train_test_split . device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) device . device(type=&#39;cpu&#39;) . Define path to dataset . DATA_PATH = &#39;data/wine.csv&#39; . Dataset Overview . df_base = pd.read_csv(DATA_PATH, sep=&#39;,&#39;) df_base.head() . Wine Alcohol Malic.acid Ash Acl Mg Phenols Flavanoids Nonflavanoid.phenols Proanth Color.int Hue OD Proline . 0 1 | 14.23 | 1.71 | 2.43 | 15.6 | 127 | 2.80 | 3.06 | 0.28 | 2.29 | 5.64 | 1.04 | 3.92 | 1065 | . 1 1 | 13.20 | 1.78 | 2.14 | 11.2 | 100 | 2.65 | 2.76 | 0.26 | 1.28 | 4.38 | 1.05 | 3.40 | 1050 | . 2 1 | 13.16 | 2.36 | 2.67 | 18.6 | 101 | 2.80 | 3.24 | 0.30 | 2.81 | 5.68 | 1.03 | 3.17 | 1185 | . 3 1 | 14.37 | 1.95 | 2.50 | 16.8 | 113 | 3.85 | 3.49 | 0.24 | 2.18 | 7.80 | 0.86 | 3.45 | 1480 | . 4 1 | 13.24 | 2.59 | 2.87 | 21.0 | 118 | 2.80 | 2.69 | 0.39 | 1.82 | 4.32 | 1.04 | 2.93 | 735 | . cols = df_base.columns . Build Data Loader . def load_and_standardize_data(path): # read in from csv df = pd.read_csv(path, sep=&#39;,&#39;) # replace nan with -99 df = df.fillna(-99) df = df.values.reshape(-1, df.shape[1]).astype(&#39;float32&#39;) # randomly split X_train, X_test = train_test_split(df, test_size=0.3, random_state=42) # standardize values scaler = preprocessing.StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) return X_train, X_test, scaler . from torch.utils.data import Dataset, DataLoader class DataBuilder(Dataset): def __init__(self, path, train=True): self.X_train, self.X_test, self.standardizer = load_and_standardize_data(DATA_PATH) if train: self.x = torch.from_numpy(self.X_train) self.len=self.x.shape[0] else: self.x = torch.from_numpy(self.X_test) self.len=self.x.shape[0] del self.X_train del self.X_test def __getitem__(self,index): return self.x[index] def __len__(self): return self.len . traindata_set=DataBuilder(DATA_PATH, train=True) testdata_set=DataBuilder(DATA_PATH, train=False) trainloader=DataLoader(dataset=traindata_set,batch_size=1024) testloader=DataLoader(dataset=testdata_set,batch_size=1024) . type(trainloader.dataset.x), type(testloader.dataset.x) . (torch.Tensor, torch.Tensor) . trainloader.dataset.x.shape, testloader.dataset.x.shape . (torch.Size([124, 14]), torch.Size([54, 14])) . trainloader.dataset.x . tensor([[ 1.3598, 0.6284, 1.0812, ..., -0.6414, -1.0709, -0.5182], [ 0.0628, -0.5409, -0.6130, ..., 0.3465, 1.3308, -0.2151], [ 0.0628, -0.7557, -1.2870, ..., 0.4324, -0.3984, 0.0420], ..., [-1.2343, 1.6904, -0.4855, ..., 1.0338, 0.5485, 2.6682], [ 0.0628, -0.3261, -0.7952, ..., 0.0029, -0.7415, -0.7983], [ 0.0628, -0.7437, 0.0428, ..., -0.6843, 1.0700, -0.9861]]) . trainloader.dataset.standardizer.inverse_transform . &lt;bound method StandardScaler.inverse_transform of StandardScaler()&gt; . Build model . class Autoencoder(nn.Module): def __init__(self,D_in,H=50,H2=12,latent_dim=3): #Encoder super(Autoencoder,self).__init__() self.linear1=nn.Linear(D_in,H) self.lin_bn1 = nn.BatchNorm1d(num_features=H) self.linear2=nn.Linear(H,H2) self.lin_bn2 = nn.BatchNorm1d(num_features=H2) self.linear3=nn.Linear(H2,H2) self.lin_bn3 = nn.BatchNorm1d(num_features=H2) # # Latent vectors mu and sigma self.fc1 = nn.Linear(H2, latent_dim) self.bn1 = nn.BatchNorm1d(num_features=latent_dim) self.fc21 = nn.Linear(latent_dim, latent_dim) self.fc22 = nn.Linear(latent_dim, latent_dim) # # Sampling vector self.fc3 = nn.Linear(latent_dim, latent_dim) self.fc_bn3 = nn.BatchNorm1d(latent_dim) self.fc4 = nn.Linear(latent_dim, H2) self.fc_bn4 = nn.BatchNorm1d(H2) # # Decoder self.linear4=nn.Linear(H2,H2) self.lin_bn4 = nn.BatchNorm1d(num_features=H2) self.linear5=nn.Linear(H2,H) self.lin_bn5 = nn.BatchNorm1d(num_features=H) self.linear6=nn.Linear(H,D_in) self.lin_bn6 = nn.BatchNorm1d(num_features=D_in) self.relu = nn.ReLU() def encode(self, x): lin1 = self.relu(self.lin_bn1(self.linear1(x))) lin2 = self.relu(self.lin_bn2(self.linear2(lin1))) lin3 = self.relu(self.lin_bn3(self.linear3(lin2))) fc1 = F.relu(self.bn1(self.fc1(lin3))) r1 = self.fc21(fc1) r2 = self.fc22(fc1) return r1, r2 def reparameterize(self, mu, logvar): if self.training: std = logvar.mul(0.5).exp_() eps = Variable(std.data.new(std.size()).normal_()) return eps.mul(std).add_(mu) else: return mu def decode(self, z): fc3 = self.relu(self.fc_bn3(self.fc3(z))) fc4 = self.relu(self.fc_bn4(self.fc4(fc3))) lin4 = self.relu(self.lin_bn4(self.linear4(fc4))) lin5 = self.relu(self.lin_bn5(self.linear5(lin4))) return self.lin_bn6(self.linear6(lin5)) def forward(self, x): mu, logvar = self.encode(x) z = self.reparameterize(mu, logvar) # self.decode(z) ist später recon_batch, mu ist mu und logvar ist logvar return self.decode(z), mu, logvar . class customLoss(nn.Module): def __init__(self): super(customLoss, self).__init__() self.mse_loss = nn.MSELoss(reduction=&quot;sum&quot;) # x_recon ist der im forward im Model erstellte recon_batch, x ist der originale x Batch, mu ist mu und logvar ist logvar def forward(self, x_recon, x, mu, logvar): loss_MSE = self.mse_loss(x_recon, x) loss_KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) return loss_MSE + loss_KLD . # takes in a module and applies the specified weight initialization def weights_init_uniform_rule(m): classname = m.__class__.__name__ # for every Linear layer in a model.. if classname.find(&#39;Linear&#39;) != -1: # get the number of the inputs n = m.in_features y = 1.0/np.sqrt(n) m.weight.data.uniform_(-y, y) m.bias.data.fill_(0) . If you want to better understand the variational autoencoder technique, look here. . For better understanding this AutoencoderClass, let me go briefly through it. This is a variational autoencoder (VAE) with two hidden layers, which (by default, but you can change this) 50 and then 12 activations. The latent factors are set to 3 (you can change that, too). So we&#39;re first exploding our initially 14 variables to 50 activations, then condensing it to 12, then to 3. From these 3 latent factors we then sample to recreate the original 14 values. We do that by inflating the 3 latent factors back to 12, then 50 and finally 14 activations (we decode the latent factors so to speak). With this reconstructed batch (recon_batch) we compare it with the original batch, computate our loss and adjust the weights and biases via our gradient (our optimizer here will be Adam). . D_in = data_set.x.shape[1] H = 50 H2 = 12 model = Autoencoder(D_in, H, H2).to(device) optimizer = optim.Adam(model.parameters(), lr=1e-3) . loss_mse = customLoss() . Train Model . epochs = 1500 log_interval = 50 val_losses = [] train_losses = [] test_losses = [] . def train(epoch): model.train() train_loss = 0 for batch_idx, data in enumerate(trainloader): data = data.to(device) optimizer.zero_grad() recon_batch, mu, logvar = model(data) loss = loss_mse(recon_batch, data, mu, logvar) loss.backward() train_loss += loss.item() optimizer.step() if epoch % 200 == 0: print(&#39;====&gt; Epoch: {} Average training loss: {:.4f}&#39;.format( epoch, train_loss / len(trainloader.dataset))) train_losses.append(train_loss / len(trainloader.dataset)) . def test(epoch): with torch.no_grad(): test_loss = 0 for batch_idx, data in enumerate(testloader): data = data.to(device) optimizer.zero_grad() recon_batch, mu, logvar = model(data) loss = loss_mse(recon_batch, data, mu, logvar) test_loss += loss.item() if epoch % 200 == 0: print(&#39;====&gt; Epoch: {} Average test loss: {:.4f}&#39;.format( epoch, test_loss / len(testloader.dataset))) test_losses.append(test_loss / len(testloader.dataset)) . for epoch in range(1, epochs + 1): train(epoch) test(epoch) . ====&gt; Epoch: 200 Average training loss: 12.3501 ====&gt; Epoch: 200 Average test loss: 11.7777 ====&gt; Epoch: 400 Average training loss: 10.1168 ====&gt; Epoch: 400 Average test loss: 8.9987 ====&gt; Epoch: 600 Average training loss: 9.2956 ====&gt; Epoch: 600 Average test loss: 9.3548 ====&gt; Epoch: 800 Average training loss: 8.9570 ====&gt; Epoch: 800 Average test loss: 8.9647 ====&gt; Epoch: 1000 Average training loss: 8.6688 ====&gt; Epoch: 1000 Average test loss: 8.5866 ====&gt; Epoch: 1200 Average training loss: 8.3341 ====&gt; Epoch: 1200 Average test loss: 8.8371 ====&gt; Epoch: 1400 Average training loss: 8.4063 ====&gt; Epoch: 1400 Average test loss: 8.7891 . We we&#39;re able to reduce the training and test loss but quite a bit, let&#39;s have a look at how the fake results actually look like vs the real results: . with torch.no_grad(): for batch_idx, data in enumerate(testloader): data = data.to(device) optimizer.zero_grad() recon_batch, mu, logvar = model(data) . scaler = trainloader.dataset.standardizer recon_row = scaler.inverse_transform(recon_batch[0].cpu().numpy()) real_row = scaler.inverse_transform(testloader.dataset.x[0].cpu().numpy()) . df = pd.DataFrame(np.stack((recon_row, real_row)), columns = cols) df . Wine Alcohol Malic.acid Ash Acl Mg Phenols Flavanoids Nonflavanoid.phenols Proanth Color.int Hue OD Proline . 0 1.002792 | 13.535107 | 2.010303 | 2.557292 | 18.198132 | 112.606842 | 2.737524 | 2.807587 | 0.320866 | 1.738254 | 4.899318 | 1.078039 | 3.187276 | 1013.391479 | . 1 1.000000 | 13.640000 | 3.100000 | 2.560000 | 15.200000 | 116.000000 | 2.700000 | 3.030000 | 0.170000 | 1.660000 | 5.100000 | 0.960000 | 3.360000 | 845.000000 | . Not to bad right (the first row is the reconstructed row, the second one the real row from the data)? However, what we want is to built this row not with the real input so to speak, since right now we were giving the model the complete rows with their 14 columns, condensed it to 3 input parameters, just to blow it up again to the corresponding 14 columns. What I want to do is to create these 14 rows by giving the model 3 latent factors as input. Let&#39;s have a look at these latent variables. . sigma = torch.exp(logvar/2) . mu[1], sigma[1] . (tensor([-0.9960, -0.8502, -0.0043]), tensor([0.2555, 0.4801, 0.9888])) . Mu represents the mean for each of our latent factor values, logvar the log of the standard deviation. Each of these have a distribution by itself. We have 54 cases in our test data, so we have 3x54 different mu and logvar. We can have a look at the distribution of each of the 3 latent variables: . mu.mean(axis=0), sigma.mean(axis=0) . (tensor([-0.0088, 0.0051, 0.0044]), tensor([0.4514, 0.3897, 0.9986])) . All of the latent variables have a mean around zero, but the last latent factor has a wider standard deviation. So when we sample values from each of these latent variables, the last value will vary much more then the other two. I assume a normal distribution for all the latent factors. . # sample z from q no_samples = 20 q = torch.distributions.Normal(mu.mean(axis=0), sigma.mean(axis=0)) z = q.rsample(sample_shape=torch.Size([no_samples])) . z.shape . torch.Size([20, 3]) . z[:5] . tensor([[ 0.5283, 0.4519, 0.6792], [ 0.3664, -0.5569, -0.1531], [-0.5802, 0.4394, 1.8406], [-1.0136, -0.4239, 0.4524], [-0.0605, 0.3913, 0.8030]]) . With these three latent factors we can now start and create fake data for our dataset and see how it looks like: . with torch.no_grad(): pred = model.decode(z).cpu().numpy() . pred[1] . array([-0.24290268, -0.6087041 , -0.44325534, -0.7158908 , -0.15065292, -0.47845733, 0.26319185, 0.23732403, -0.22809544, 0.12187037, -0.8295655 , 0.44908378, 0.6173717 , -0.55648965], dtype=float32) . Create fake data from Autoencoder . fake_data = scaler.inverse_transform(pred) fake_data.shape . (20, 14) . df_fake = pd.DataFrame(fake_data, columns = cols) df_fake[&#39;Wine&#39;] = np.round(df_fake[&#39;Wine&#39;]).astype(int) df_fake[&#39;Wine&#39;] = np.where(df_fake[&#39;Wine&#39;]&lt;1, 1, df_fake[&#39;Wine&#39;]) df_fake.head(10) . Wine Alcohol Malic.acid Ash Acl Mg Phenols Flavanoids Nonflavanoid.phenols Proanth Color.int Hue OD Proline . 0 3 | 13.350755 | 3.817283 | 2.425754 | 21.229387 | 98.816788 | 1.682916 | 0.910786 | 0.450081 | 1.245882 | 8.242197 | 0.667928 | 1.705379 | 636.650818 | . 1 2 | 12.453159 | 1.916350 | 2.172731 | 18.977226 | 93.556114 | 2.444676 | 2.246270 | 0.335432 | 1.663583 | 3.166457 | 1.063876 | 3.050176 | 568.385925 | . 2 2 | 12.735057 | 2.404566 | 2.447556 | 20.400013 | 105.475235 | 1.937112 | 1.657119 | 0.385740 | 1.452577 | 4.242754 | 0.928397 | 2.467263 | 680.271545 | . 3 1 | 14.664644 | 1.517465 | 2.269279 | 12.428186 | 88.851791 | 3.354010 | 3.997237 | 0.265253 | 2.586414 | 7.366968 | 1.275564 | 3.170231 | 1516.662720 | . 4 3 | 13.160161 | 3.359397 | 2.415784 | 21.050211 | 99.859154 | 1.662516 | 0.929189 | 0.427978 | 1.135361 | 7.101127 | 0.708510 | 1.732820 | 640.412231 | . 5 2 | 12.453159 | 1.916350 | 2.172731 | 18.977226 | 93.556114 | 2.444676 | 2.246270 | 0.335432 | 1.663583 | 3.166457 | 1.063876 | 3.050176 | 568.385925 | . 6 2 | 12.520310 | 2.522696 | 2.375254 | 20.435560 | 92.619812 | 1.838333 | 1.361269 | 0.470815 | 1.221076 | 4.518130 | 0.906680 | 2.146883 | 583.079102 | . 7 3 | 12.877177 | 2.746192 | 2.395865 | 20.154610 | 97.263092 | 1.744550 | 1.187050 | 0.464942 | 1.160733 | 5.619783 | 0.836708 | 1.871472 | 665.485718 | . 8 2 | 12.679532 | 2.344776 | 2.331834 | 19.901327 | 97.031586 | 1.857117 | 1.495742 | 0.461352 | 1.239715 | 4.668478 | 0.934352 | 2.094139 | 680.778809 | . 9 2 | 13.062141 | 2.719065 | 2.461590 | 19.947014 | 103.352890 | 2.070540 | 1.566055 | 0.380154 | 1.293219 | 5.675068 | 0.852832 | 2.128047 | 778.582825 | . For comparison the real data: . df_base.sample(10) . Wine Alcohol Malic.acid Ash Acl Mg Phenols Flavanoids Nonflavanoid.phenols Proanth Color.int Hue OD Proline . 1 1 | 13.20 | 1.78 | 2.14 | 11.2 | 100 | 2.65 | 2.76 | 0.26 | 1.28 | 4.38 | 1.05 | 3.40 | 1050 | . 35 1 | 13.48 | 1.81 | 2.41 | 20.5 | 100 | 2.70 | 2.98 | 0.26 | 1.86 | 5.10 | 1.04 | 3.47 | 920 | . 114 2 | 12.08 | 1.39 | 2.50 | 22.5 | 84 | 2.56 | 2.29 | 0.43 | 1.04 | 2.90 | 0.93 | 3.19 | 385 | . 149 3 | 13.08 | 3.90 | 2.36 | 21.5 | 113 | 1.41 | 1.39 | 0.34 | 1.14 | 9.40 | 0.57 | 1.33 | 550 | . 158 3 | 14.34 | 1.68 | 2.70 | 25.0 | 98 | 2.80 | 1.31 | 0.53 | 2.70 | 13.00 | 0.57 | 1.96 | 660 | . 9 1 | 13.86 | 1.35 | 2.27 | 16.0 | 98 | 2.98 | 3.15 | 0.22 | 1.85 | 7.22 | 1.01 | 3.55 | 1045 | . 90 2 | 12.08 | 1.83 | 2.32 | 18.5 | 81 | 1.60 | 1.50 | 0.52 | 1.64 | 2.40 | 1.08 | 2.27 | 480 | . 47 1 | 13.90 | 1.68 | 2.12 | 16.0 | 101 | 3.10 | 3.39 | 0.21 | 2.14 | 6.10 | 0.91 | 3.33 | 985 | . 10 1 | 14.10 | 2.16 | 2.30 | 18.0 | 105 | 2.95 | 3.32 | 0.22 | 2.38 | 5.75 | 1.25 | 3.17 | 1510 | . 31 1 | 13.58 | 1.66 | 2.36 | 19.1 | 106 | 2.86 | 3.19 | 0.22 | 1.95 | 6.90 | 1.09 | 2.88 | 1515 | . Compare variables grouped by Wine . df_base.groupby(&#39;Wine&#39;).mean() . Alcohol Malic.acid Ash Acl Mg Phenols Flavanoids Nonflavanoid.phenols Proanth Color.int Hue OD Proline . Wine . 1 13.744746 | 2.010678 | 2.455593 | 17.037288 | 106.338983 | 2.840169 | 2.982373 | 0.290000 | 1.899322 | 5.528305 | 1.062034 | 3.157797 | 1115.711864 | . 2 12.278732 | 1.932676 | 2.244789 | 20.238028 | 94.549296 | 2.258873 | 2.080845 | 0.363662 | 1.630282 | 3.086620 | 1.056282 | 2.785352 | 519.507042 | . 3 13.153750 | 3.333750 | 2.437083 | 21.416667 | 99.312500 | 1.678750 | 0.781458 | 0.447500 | 1.153542 | 7.396250 | 0.682708 | 1.683542 | 629.895833 | . df_fake.groupby(&#39;Wine&#39;).mean() . Alcohol Malic.acid Ash Acl Mg Phenols Flavanoids Nonflavanoid.phenols Proanth Color.int Hue OD Proline . Wine . 1 13.812141 | 1.814212 | 2.482638 | 17.172688 | 107.468864 | 3.062387 | 3.344664 | 0.259955 | 2.162966 | 5.331643 | 1.147217 | 3.280716 | 1148.031372 | . 2 12.560544 | 2.157595 | 2.301805 | 19.696327 | 99.324005 | 2.254415 | 1.995140 | 0.366076 | 1.575015 | 3.791955 | 1.000527 | 2.741598 | 629.895203 | . 3 13.170316 | 3.413856 | 2.416369 | 20.929930 | 99.028229 | 1.683604 | 0.964315 | 0.443444 | 1.176529 | 7.288512 | 0.718357 | 1.745200 | 644.870056 | . That looks pretty convincing if you ask me. . To sum up, we&#39;ve built a variational autoencoder, which we trained on our trainingset. We checked whether our loss kept on improving based on the testset, which the autoencoder never saw for generating fake data. We then calculated the mean and standard deviation from our latent factors given the test data. We&#39;ve then sampled from this distribution to feed it back into our decoder to create some fake data. With this approach I am now able to create as much fake data derived from the underlying distribution as a want. And I think the results look promising. . You can take this approach to for example create data from under-represented in highly skewed datasets instead of just weighting them higher. The re-weighting approach might cause the algorithm to find relations where there are none, only because a few then overrepresented data points share this relation by random. With the shown approach, the learned distribution would take into account the high variance these features have and therefore will hopefully help the algorithm to not draw these false conclusions. . Stay tuned for the next blogpost, where I will show the shown approach in exactly this use case. .",
            "url": "https://lschmiddey.github.io/fastpages_/2021/03/14/tabular-data-variational-autoencoder.html",
            "relUrl": "/2021/03/14/tabular-data-variational-autoencoder.html",
            "date": " • Mar 14, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "MLP Tutorial",
            "content": "How to make use of the ml-prepare package . import mlprepare as mlp import pandas as pd import numpy . Load Data . df = pd.read_csv(&#39;TrainAndValid.csv&#39;, low_memory=False) . df.head() . SalesID SalePrice MachineID ModelID datasource auctioneerID YearMade MachineHoursCurrentMeter UsageBand saledate ... Undercarriage_Pad_Width Stick_Length Thumb Pattern_Changer Grouser_Type Backhoe_Mounting Blade_Type Travel_Controls Differential_Type Steering_Controls . 0 1139246 | 66000.0 | 999089 | 3157 | 121 | 3.0 | 2004 | 68.0 | Low | 11/16/2006 0:00 | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Standard | Conventional | . 1 1139248 | 57000.0 | 117657 | 77 | 121 | 3.0 | 1996 | 4640.0 | Low | 3/26/2004 0:00 | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Standard | Conventional | . 2 1139249 | 10000.0 | 434808 | 7009 | 121 | 3.0 | 2001 | 2838.0 | High | 2/26/2004 0:00 | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 1139251 | 38500.0 | 1026470 | 332 | 121 | 3.0 | 2001 | 3486.0 | High | 5/19/2011 0:00 | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 1139253 | 11000.0 | 1057373 | 17311 | 121 | 3.0 | 2007 | 722.0 | Medium | 7/23/2009 0:00 | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 5 rows × 53 columns . to_keep = [&#39;SalePrice&#39;, &#39;MachineID&#39;, &#39;saledate&#39;, &#39;MachineHoursCurrentMeter&#39;, &#39;UsageBand&#39;] df = df[to_keep] df.head() . SalePrice MachineID saledate MachineHoursCurrentMeter UsageBand . 0 66000.0 | 999089 | 11/16/2006 0:00 | 68.0 | Low | . 1 57000.0 | 117657 | 3/26/2004 0:00 | 4640.0 | Low | . 2 10000.0 | 434808 | 2/26/2004 0:00 | 2838.0 | High | . 3 38500.0 | 1026470 | 5/19/2011 0:00 | 3486.0 | High | . 4 11000.0 | 1057373 | 7/23/2009 0:00 | 722.0 | Medium | . mlp Functions . df_to_type . date_type = [&#39;saledate&#39;] continuous_type = [&#39;SalePrice&#39;, &#39;MachineHoursCurrentMeter&#39;] categorical_type = [&#39;MachineID&#39;, &#39;UsageBand&#39;] . result = mlp.df_to_type(df, date_type, continuous_type, categorical_type) . result.head() . SalePrice MachineID saleWeek MachineHoursCurrentMeter UsageBand saleYear saleMonth saleDay saleDayofweek saleDayofyear saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start saleElapsed . 0 66000.0 | 999089 | 46 | 68.0 | Low | 2006 | 11 | 16 | 3 | 320 | False | False | False | False | False | False | 1163635200 | . 1 57000.0 | 117657 | 13 | 4640.0 | Low | 2004 | 3 | 26 | 4 | 86 | False | False | False | False | False | False | 1080259200 | . 2 10000.0 | 434808 | 9 | 2838.0 | High | 2004 | 2 | 26 | 3 | 57 | False | False | False | False | False | False | 1077753600 | . 3 38500.0 | 1026470 | 20 | 3486.0 | High | 2011 | 5 | 19 | 3 | 139 | False | False | False | False | False | False | 1305763200 | . 4 11000.0 | 1057373 | 30 | 722.0 | Medium | 2009 | 7 | 23 | 3 | 204 | False | False | False | False | False | False | 1248307200 | . We automatically extracted some extra information from the date variable and transformed the categorical variables to the correct type. . result.dtypes . SalePrice float64 MachineID category saleWeek UInt32 MachineHoursCurrentMeter float64 UsageBand category saleYear int64 saleMonth int64 saleDay int64 saleDayofweek int64 saleDayofyear int64 saleIs_month_end bool saleIs_month_start bool saleIs_quarter_end bool saleIs_quarter_start bool saleIs_year_end bool saleIs_year_start bool saleElapsed object dtype: object . Let&#39;s only keep the saleYear and saleMonth from our date variable. . to_keep = [&#39;SalePrice&#39;, &#39;MachineID&#39;, &#39;MachineHoursCurrentMeter&#39;, &#39;UsageBand&#39;, &#39;saleYear&#39;, &#39;saleMonth&#39;] continuous_type = [&#39;SalePrice&#39;, &#39;MachineHoursCurrentMeter&#39;] categorical_type = [&#39;MachineID&#39;, &#39;UsageBand&#39;] result = result[to_keep] result.head() . SalePrice MachineID MachineHoursCurrentMeter UsageBand saleYear saleMonth . 0 66000.0 | 999089 | 68.0 | Low | 2006 | 11 | . 1 57000.0 | 117657 | 4640.0 | Low | 2004 | 3 | . 2 10000.0 | 434808 | 2838.0 | High | 2004 | 2 | . 3 38500.0 | 1026470 | 3486.0 | High | 2011 | 5 | . 4 11000.0 | 1057373 | 722.0 | Medium | 2009 | 7 | . split_df . Now, let&#39;s split the data into train and test, first randomly, then by a variable, then by a condition. . X_train, X_test, y_train, y_test = mlp.split_df(result, dep_var=&#39;SalePrice&#39;, test_size=0.3, split_mode=&#39;random&#39;) . X_train.shape, X_test.shape . ((288888, 5), (123810, 5)) . X_train, X_test, y_train, y_test = mlp.split_df(result, dep_var=&#39;SalePrice&#39;, test_size=0.3, split_mode=&#39;on_split_id&#39;, split_var=&#39;MachineID&#39;) . X_train.shape, X_test.shape . ((276523, 5), (136175, 5)) . #every row that fulfills this condition will be in the trainset cond = (result.saleYear&lt;2009) . X_train, X_test, y_train, y_test = mlp.split_df(result, dep_var=&#39;SalePrice&#39;, test_size=0.3, split_mode=&#39;on_condition&#39;, cond=cond) . X_train.shape, X_test.shape . ((288689, 5), (124009, 5)) . X_train.head() . MachineID MachineHoursCurrentMeter UsageBand saleYear saleMonth . 0 999089 | 68.0 | Low | 2006 | 11 | . 1 117657 | 4640.0 | Low | 2004 | 3 | . 2 434808 | 2838.0 | High | 2004 | 2 | . 5 1001274 | 508.0 | Low | 2008 | 12 | . 6 772701 | 11540.0 | High | 2004 | 8 | . cat_transform . X_train_, X_test_, dict_list, dict_inv_list = mlp.cat_transform(X_train, X_test, cat_type = categorical_type) . X_train_.head() . MachineID MachineHoursCurrentMeter UsageBand saleYear saleMonth . 0 62273 | 68.0 | 2 | 2006 | 11 | . 1 9581 | 4640.0 | 2 | 2004 | 3 | . 2 28730 | 2838.0 | 1 | 2004 | 2 | . 5 62837 | 508.0 | 2 | 2008 | 12 | . 6 48637 | 11540.0 | 1 | 2004 | 8 | . We changed the defined categorical types to int and saved the corresponding dictionaries. Also, we added a special token for NaN values. . dict_list[1] . {0: &#39;#NaN&#39;, 1: &#39;High&#39;, 2: &#39;Low&#39;, 3: &#39;Medium&#39;} . cont_standardize . Let&#39;s standardize the data. If we want specific columns to not be standardized, we can put them into the cat_type argument. If we have an ID to later match the results to, put it into the id_type argument and it will not be standardized. If you don&#39;t want the dependend variable to be standardized, set transform_y to False (also realize that you will not get the scaler_y object as an output). . categorical_type = [&#39;MachineID&#39;, &#39;UsageBand&#39;, &#39;saleYear&#39;, &#39;saleMonth&#39;] X_train_2, X_test_2, y_train_2, y_test_2, scaler, scaler_y = mlp.cont_standardize(X_train_, X_test_, y_train, y_test, cat_type=categorical_type, transform_y=True, path=&#39;&#39;, standardizer=&#39;StandardScaler&#39;) . y_train[:5] . 0 66000.0 1 57000.0 2 10000.0 5 26500.0 6 21000.0 Name: SalePrice, dtype: float64 . y_train_2[:5] . array([[ 1.53656444], [ 1.14173911], [-0.92012647], [-0.19628004], [-0.43756218]]) . X_train_2.head() . MachineID MachineHoursCurrentMeter UsageBand saleYear saleMonth . 0 62273 | -0.372846 | 2 | 2006 | 11 | . 1 9581 | 0.893532 | 2 | 2004 | 3 | . 2 28730 | 0.394404 | 1 | 2004 | 2 | . 5 62837 | -0.250972 | 2 | 2008 | 12 | . 6 48637 | 2.804732 | 1 | 2004 | 8 | . saleYear and saleMonth didn&#39;t get standardized, also the categorical variables didn&#39;t get standardized. .",
            "url": "https://lschmiddey.github.io/fastpages_/2021/02/01/mlp-Tutorial.html",
            "relUrl": "/2021/02/01/mlp-Tutorial.html",
            "date": " • Feb 1, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "How to build a web app with Streamlit",
            "content": "Combine Streamlit and Heroku to easily build and deploy a machine learning web-app . In this blog post I want to show how easy it is to build a web application with Streamlit and then use Heroku to deploy it and make it accessible for everyone. For this small web-app, I want to detect blobs in a given image. Furthermore, I want to be able to fiddle around with the parameters within the model and see the results immediately on the webpage. So let&#39;s first start with building our python program to detect blobs. . For this, we will make use of opencv-python, which already has a class with which we can detect blobs in images: the SimpleBlobDetector . import cv2 blob_params = cv2.SimpleBlobDetector_Params() blob_params.filterByInertia = False blob_params.filterByConvexity = False blob_params.filterByColor = True blob_params.blobColor = 0 blob_params.filterByCircularity = True blob_params.filterByArea = False blob_detector = cv2.SimpleBlobDetector_create(blob_params) . These are some of the parameters the class has got. Next, we fetch an image and use the algorithm to detect the biggest blob detected. . keypoints = blob_detector.detect(openCVim) # find largest blob if len(keypoints) &gt; 0: kp_max = keypoints[0] for kp in keypoints: if kp.size &gt; kp_max.size: kp_max = kp . And that&#39;s pretty much it. openCVim will be the input of our model, and kp_max the keypoint with the biggest size. What we need now is streamlit to easily create a webpage which will use this model. . import streamlit as st st.write(&quot;&quot;&quot; # Simple Blob Detection App Upload your image and see where the Blob is! &quot;&quot;&quot;) st.sidebar.header(&#39;User Input Parameters&#39;) . It&#39;s that easy. We created a header for our webpage and a sidebar with a different header. In this sidebar, we want to be able to fiddle around with some of the blob_params. . def user_input_features(): minCircularity = st.sidebar.slider(&#39;minCircularity&#39;, 0., 1., 0.8) minArea = st.sidebar.slider(&#39;minArea&#39;, 0, 1, 10) maxArea = st.sidebar.slider(&#39;maxArea&#39;, 0, 1, 100000) data = {&#39;minCircularity&#39;: minCircularity, &#39;minArea&#39;: minArea, &#39;maxArea&#39;: maxArea} features = pd.DataFrame(data, index=[0]) return features df = user_input_features() blob_params.minCircularity = df.minCircularity.values.item() blob_params.minArea = df.minArea.values.item() blob_params.maxArea = df.maxArea.values.item() st.subheader(&#39;User Input parameters&#39;) st.write(df) . We defined a function, which uses streamlits sidebar.slider function. The first value is the lowest value the slider can be, the second the largest value the slider can be and the last value is the default value. We then use these values to create a dataframe, which we use to fill the blob_params. So each time the user changes one of the values via the slider, the model get&#39;s a different parameter input and will automatically rerun the model. With st.subheader we give a subheader and then show the dataframe to the user with the chosen parameters. . uploaded_file = st.file_uploader(&quot;Choose an image...&quot;, type=&quot;jpg&quot;) if uploaded_file is not None: # Read in and make greyscale PILim = Image.open(uploaded_file).convert(&#39;L&#39;) # Make Numpy/OpenCV-compatible version openCVim = np.array(PILim) openCVim = cv2.bitwise_not(openCVim) st.image(openCVim, caption=&#39;Uploaded Image.&#39;, use_column_width=True) . Next, the user should upload an image. Only when this file is not None we will proceed with our modeling. We read the image in with PIL, put it to grayscale (in my experience for blob-detection that worked better) and transform it to an numpy array, which cv2 needs to create the model. . keypoints = blob_detector.detect(openCVim) # find largest blob if len(keypoints) &gt; 0: kp_max = keypoints[0] for kp in keypoints: if kp.size &gt; kp_max.size: kp_max = kp pts = np.array([kp_max.pt]) data_coordinates = {&#39;x_coordinate&#39;: int(pts[:, 0]), &#39;y_coordinate&#39;: int(pts[:, 1])} df_coordinates = pd.DataFrame(data_coordinates, index=[0]) im_with_keypoints = cv2.cvtColor(openCVim,cv2.COLOR_GRAY2RGB) # im_with_keypoints = cv2.circle(openCVim, (int(pts[:, 0]), int(pts[:, 1])), 50, color=(0,255,0), thickness=30, lineType=8, shift=0) im_with_keypoints = cv2.drawKeypoints(im_with_keypoints, [kp_max], np.array([]), (0,0,255), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS) st.image(im_with_keypoints, caption=&#39;Image with Blob.&#39;, use_column_width=True) st.write(df_coordinates) . Still in the if statement we progress by building the model, get the biggest blob and get the coordinates by calling kp_max.pt. We store these values in a dictionary to put it in a dataframe, which we show at the end of the page. We also use the coordinates to mark the calculated keypoint. Finally, we return the image with a circle around the detected image to the user. . And that&#39;s it, we build a fully functional web-app with streamlit. Finally, we want to deploy it on Heroku. After creating an account it should look something like this: . . Click on &quot;Create new App&quot;. . . . Use your git-repository to let Heroku deploy the web-app for you, click on connect and your done! It&#39;s that easy. Heroku requires three more files in your repository: . ⋅⋅ requirements.txt, where you specify the packages you use and the respective version ⋅⋅ Procfile, which tells Heroku how to start the Streamlit-app ⋅⋅* setup.sh, which is a shell script which tells the Heroku server how to deploy the Streamlit-app . Simply check out my git repository for an example. . After all is said and done, this is how the app looks like: . . . . Or simply check out the web-page. . I hope you enjoyed this post and stay tuned for the next one. . Lasse .",
            "url": "https://lschmiddey.github.io/fastpages_/2021/01/24/How-to-build-web-app-with-streamlit.html",
            "relUrl": "/2021/01/24/How-to-build-web-app-with-streamlit.html",
            "date": " • Jan 24, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "How to pre-process data for machine learning",
            "content": "A common task for almost all data science projects is to preprocess your data. This is especially the case for tabular data. In this blog I want to build a pre-process function which can handle will handle all these pre-processing steps for us. On top of that, I will add enough flexibility to that function to make it useful not just in this example. . Let&#39;s start by loading the data. . Imports . import numpy as np import pandas as pd import pickle from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, MinMaxScaler . We later want to save and load data with pickle, so let&#39;s define these functions: . def save_obj(obj, name ): with open(f&#39;{name}.pkl&#39;, &#39;wb&#39;) as f: pickle.dump(obj, f) def load_obj(name ): with open(f&#39;{name}.pkl&#39;, &#39;rb&#39;) as f: return pickle.load(f) . df = pd.read_csv(&#39;train.csv&#39;) . df.head() . Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition SalePrice . 0 1 | 60 | RL | 65.0 | 8450 | Pave | NaN | Reg | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 2 | 2008 | WD | Normal | 208500 | . 1 2 | 20 | RL | 80.0 | 9600 | Pave | NaN | Reg | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 5 | 2007 | WD | Normal | 181500 | . 2 3 | 60 | RL | 68.0 | 11250 | Pave | NaN | IR1 | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 9 | 2008 | WD | Normal | 223500 | . 3 4 | 70 | RL | 60.0 | 9550 | Pave | NaN | IR1 | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 2 | 2006 | WD | Abnorml | 140000 | . 4 5 | 60 | RL | 84.0 | 14260 | Pave | NaN | IR1 | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 12 | 2008 | WD | Normal | 250000 | . 5 rows × 81 columns . df.shape . (1460, 81) . We&#39;ve got 81 columns in our data. That&#39;s a lot of data, for this blog we only need a couple of these variables. The function will then work for any length of datasets. We define which columns to keep and then check the data types. . to_keep = [&#39;Id&#39;, &#39;MSZoning&#39;, &#39;LotFrontage&#39;, &#39;LotArea&#39;, &#39;Street&#39;, &#39;OverallQual&#39;, &#39;YearBuilt&#39;, &#39;CentralAir&#39;, &#39;SaleType&#39;, &#39;SalePrice&#39;] . df = df[to_keep] df.head() . Id MSZoning LotFrontage LotArea Street OverallQual YearBuilt CentralAir SaleType SalePrice . 0 1 | RL | 65.0 | 8450 | Pave | 7 | 2003 | Y | WD | 208500 | . 1 2 | RL | 80.0 | 9600 | Pave | 6 | 1976 | Y | WD | 181500 | . 2 3 | RL | 68.0 | 11250 | Pave | 7 | 2001 | Y | WD | 223500 | . 3 4 | RL | 60.0 | 9550 | Pave | 7 | 1915 | Y | WD | 140000 | . 4 5 | RL | 84.0 | 14260 | Pave | 8 | 2000 | Y | WD | 250000 | . pd.set_option(&#39;display.max_rows&#39;, 20) df.dtypes . Id int64 MSZoning object LotFrontage float64 LotArea int64 Street object OverallQual int64 YearBuilt int64 CentralAir object SaleType object SalePrice int64 dtype: object . Datetime vs Continuous vs Categorical . In tabular data, there are usually four different data types: dates, continuous and categorical variables (of which boolean are a special type). We need to define which column is of which type. This dataset does not contain a datetime column, so I will define some random datetime data to show you how to cope with datetime data too. . df[&quot;Fake_date&quot;] = np.random.choice(pd.date_range(&#39;1980-01-01&#39;, &#39;2000-01-01&#39;), len(df)).astype(&#39;str&#39;) df[&quot;Fake_date_2&quot;] = np.random.choice(pd.date_range(&#39;1980-01-01&#39;, &#39;2000-01-01&#39;), len(df)).astype(&#39;str&#39;) . df.head() . Id MSZoning LotFrontage LotArea Street OverallQual YearBuilt CentralAir SaleType SalePrice Fake_date Fake_date_2 . 0 1 | RL | 65.0 | 8450 | Pave | 7 | 2003 | Y | WD | 208500 | 1985-12-22T00:00:00.000000000 | 1988-05-22T00:00:00.000000000 | . 1 2 | RL | 80.0 | 9600 | Pave | 6 | 1976 | Y | WD | 181500 | 1999-11-12T00:00:00.000000000 | 1983-08-11T00:00:00.000000000 | . 2 3 | RL | 68.0 | 11250 | Pave | 7 | 2001 | Y | WD | 223500 | 1998-03-07T00:00:00.000000000 | 1991-12-15T00:00:00.000000000 | . 3 4 | RL | 60.0 | 9550 | Pave | 7 | 1915 | Y | WD | 140000 | 1985-04-11T00:00:00.000000000 | 1980-11-27T00:00:00.000000000 | . 4 5 | RL | 84.0 | 14260 | Pave | 8 | 2000 | Y | WD | 250000 | 1980-12-04T00:00:00.000000000 | 1982-10-01T00:00:00.000000000 | . df.dtypes . Id int64 MSZoning object LotFrontage float64 LotArea int64 Street object OverallQual int64 YearBuilt int64 CentralAir object SaleType object SalePrice int64 Fake_date object Fake_date_2 object dtype: object . We now need to define which column should be of which type. We use four lists for this: . date_type = [&#39;Fake_date&#39;, &#39;Fake_date_2&#39;] continuous_type = [&#39;LotFrontage&#39;, &#39;LotArea&#39;, &#39;YearBuilt&#39;, &#39;SalePrice&#39;] categorical_type = [&#39;MSZoning&#39;, &#39;Street&#39;, &#39;OverallQual&#39;, &#39;SaleType&#39;,&#39;CentralAir&#39;] . I defined &#39;OverallQual&#39; as a category, because even though this variable is in ascending order, we do not know whether the difference from OverallQual of 6 and 7 is the same magnitude than OverallQual from 0 to 1. However, you can define this variable as continous as well. . We then define a small function, which will take our lists as input and transform our data to the correct data types. . def df_to_type(df, cont_type, cat_type): if cat_type is not None: df[cat_type] = df[cat_type].astype(&#39;category&#39;) for i in date_type: df[i] = pd.to_datetime(df[i]) return df . Did it work? . df_to_type(df, continuous_type, categorical_type) df.dtypes . Id int64 MSZoning category LotFrontage float64 LotArea int64 Street category OverallQual category YearBuilt int64 CentralAir category SaleType category SalePrice int64 Fake_date datetime64[ns] Fake_date_2 datetime64[ns] dtype: object . That looks good! We can now make use of pandas various datetime functions to add more informations to our data. I therefore make use of fastai&#39;s add_datepart function. However, I do not want to import the whole library, so I simple get the parts for this specific function. . import re def ifnone(a:any,b:any)-&gt;any: &quot;`a` if `a` is not None, otherwise `b`.&quot; return b if a is None else a def make_date(df, date_field): &quot;Make sure `df[date_field]` is of the right date type.&quot; field_dtype = df[date_field].dtype if isinstance(field_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype): field_dtype = np.datetime64 if not np.issubdtype(field_dtype, np.datetime64): df[date_field] = pd.to_datetime(df[date_field], infer_datetime_format=True) def add_datepart(df, field_name, prefix=None, drop=True, time=False): &quot;Helper function that adds columns relevant to a date in the column `field_name` of `df`.&quot; make_date(df, field_name) field = df[field_name] prefix = ifnone(prefix, re.sub(&#39;[Dd]ate$&#39;, &#39;&#39;, field_name)) attr = [&#39;Year&#39;, &#39;Month&#39;, &#39;Day&#39;, &#39;Dayofweek&#39;, &#39;Dayofyear&#39;, &#39;Is_month_end&#39;, &#39;Is_month_start&#39;, &#39;Is_quarter_end&#39;, &#39;Is_quarter_start&#39;, &#39;Is_year_end&#39;, &#39;Is_year_start&#39;] if time: attr = attr + [&#39;Hour&#39;, &#39;Minute&#39;, &#39;Second&#39;] for n in attr: df[prefix + n] = getattr(field.dt, n.lower()) # Pandas removed `dt.week` in v1.1.10 week = field.dt.isocalendar().week if hasattr(field.dt, &#39;isocalendar&#39;) else field.dt.week df.insert(3, prefix+&#39;Week&#39;, week) mask = ~field.isna() df[prefix + &#39;Elapsed&#39;] = np.where(mask,field.values.astype(np.int64) // 10 ** 9,None) if drop: df.drop(field_name, axis=1, inplace=True) return df . Let&#39;s add to the function, so that after defining the correct data type it takes all of our date-variables and transforms them according to add_datepart. . def df_to_type(df, date_type=None, cont_type=None, cat_type=None): if cat_type is not None: df[cat_type] = df[cat_type].astype(&#39;category&#39;) if date_type is not None: for i in date_type: df[i] = pd.to_datetime(df[i]) df = add_datepart(df, i) return df . df_1 = df_to_type(df, date_type, continuous_type, categorical_type) df_1.head() . Id MSZoning LotFrontage Fake_date_2Week Fake_Week LotArea Street OverallQual YearBuilt CentralAir ... Fake_date_2Day Fake_date_2Dayofweek Fake_date_2Dayofyear Fake_date_2Is_month_end Fake_date_2Is_month_start Fake_date_2Is_quarter_end Fake_date_2Is_quarter_start Fake_date_2Is_year_end Fake_date_2Is_year_start Fake_date_2Elapsed . 0 1 | RL | 65.0 | 20 | 51 | 8450 | Pave | 7 | 2003 | Y | ... | 22 | 6 | 143 | False | False | False | False | False | False | 580262400 | . 1 2 | RL | 80.0 | 32 | 45 | 9600 | Pave | 6 | 1976 | Y | ... | 11 | 3 | 223 | False | False | False | False | False | False | 429408000 | . 2 3 | RL | 68.0 | 50 | 10 | 11250 | Pave | 7 | 2001 | Y | ... | 15 | 6 | 349 | False | False | False | False | False | False | 692755200 | . 3 4 | RL | 60.0 | 48 | 15 | 9550 | Pave | 7 | 1915 | Y | ... | 27 | 3 | 332 | False | False | False | False | False | False | 344131200 | . 4 5 | RL | 84.0 | 39 | 49 | 14260 | Pave | 8 | 2000 | Y | ... | 1 | 4 | 274 | False | True | False | True | False | False | 402278400 | . 5 rows × 36 columns . Pretty cool right? The next step is to split the data into train and validation set. There are usually three ways to do this: split randomly, split based on a identifier column (so that the same person is either in train or valid regardless of how many rows she represents in the data) or by date (usually in time series). . I give three examples how to do this given this specific data. I start by randomly splitting the data: . First, we need to define our dependend variable and the rest: . dep_var = &#39;SalePrice&#39; cols = list(df_1.columns) cols.remove(dep_var) . Here&#39;s how we can make a random split using the train_test_split: . X_train, X_test, y_train, y_test = train_test_split(df_1[cols], df_1[dep_var], test_size=0.33, random_state=42) . X_train.shape, X_test.shape, y_train.shape, y_test.shape . ((978, 35), (482, 35), (978,), (482,)) . Another common way how to split the data is based on an ID, so that an ID can only be in one group. Next to the dependend variable we need to define the variable on which to split. . split_var = &#39;Id&#39; . # list of unique_id unique_id_array = list(df_1[split_var].unique()) # split into train and test data based on uid test_size=0.33 cnt_uid = len(unique_id_array) len_test = np.round(cnt_uid*test_size).astype(int) len_train = cnt_uid - len_test test_idx = list(np.random.choice(unique_id_array, len_test, replace=False)) train_idx = list(set(unique_id_array) - set(test_idx)) X_train = df_1[df_1[split_var].isin(train_idx)].copy() y_train = X_train[dep_var] X_train = X_train[cols] X_test = df_1[df_1[split_var].isin(test_idx)].copy() y_test = X_test[dep_var] X_test = X_test[cols] . X_train.shape, X_test.shape, y_train.shape, y_test.shape . ((978, 35), (482, 35), (978,), (482,)) . In this case we end up with the exact same shape of all train and test data. This is due to the fact, that in this case the Id is unique. To see that this will change when there is more than one row per Id I create a fake_ID and show you the results. Let&#39;s assume we have 10 IDs: . IDS = np.array([1,2,3,4,5,6,7,8,9,10]) n = df_1.shape[0] df_1[&#39;Fake_ID&#39;] = np.resize(IDS, n) . df_1.Fake_ID.head(12) . 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 10 10 1 11 2 Name: Fake_ID, dtype: int64 . We now do the same thing as before, only using this Fake_ID as split_ID: . split_var = &#39;Fake_ID&#39; # list of unique_id unique_id_array = list(df_1[split_var].unique()) # split into train and test data based on uid test_size=0.33 cnt_uid = len(unique_id_array) len_test = np.round(cnt_uid*test_size).astype(int) len_train = cnt_uid - len_test test_idx = list(np.random.choice(unique_id_array, len_test, replace=False)) train_idx = list(set(unique_id_array) - set(test_idx)) X_train = df_1[df_1[split_var].isin(train_idx)].copy() y_train = X_train[dep_var] X_train = X_train[cols] X_test = df_1[df_1[split_var].isin(test_idx)].copy() y_test = X_test[dep_var] X_test = X_test[cols] . X_train.shape, X_test.shape, y_train.shape, y_test.shape . ((1022, 35), (438, 35), (1022,), (438,)) . Beatiful! So let&#39;s move on to our last way to split data, defined by the date. We have created the Fake_date variable, resulting in random dates from 01.01.2980 - 01.01.2000. Let&#39;s say, we want the last 6 month to be our testset. . cond = (df_1.Fake_Year&lt;1999) | (df_1.Fake_Month&lt;6) train_idx = np.where( cond)[0] test_idx = np.where(~cond)[0] X_train = df_1.iloc[train_idx] y_train = X_train[dep_var] X_train = X_train[cols] X_test = df_1.iloc[test_idx] y_test = X_test[dep_var] X_test = X_test[cols] . X_train.shape, X_test.shape, y_train.shape, y_test.shape . ((1413, 35), (47, 35), (1413,), (47,)) . We now have the three most common cases for how to split your data. Let&#39;s put that into a function. . def split_df(df, x_cols, dep_var, test_size, split_mode=&#39;random&#39;, split_var=None, cond=None): &#39;&#39;&#39; split_mode can take three values: random, on_split_id, on_condition &#39;&#39;&#39; if split_mode == &#39;random&#39;: from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(df[x_cols], df_1[dep_var], test_size=test_size) elif split_mode == &#39;on_split_id&#39;: if split_var is None: print(&#39;Give name of split_var&#39;) else: # list of unique_id unique_id_array = list(df[split_var].unique()) # split into train and test data based on uid test_size=0.33 cnt_uid = len(unique_id_array) len_test = np.round(cnt_uid*test_size).astype(int) len_train = cnt_uid - len_test test_idx = list(np.random.choice(unique_id_array, len_test, replace=False)) train_idx = list(set(unique_id_array) - set(test_idx)) X_train = df[df[split_var].isin(train_idx)].copy() y_train = X_train[dep_var] X_train = X_train[x_cols] X_test = df[df[split_var].isin(test_idx)].copy() y_test = X_test[dep_var] X_test = X_test[x_cols] elif split_mode == &#39;on_condition&#39;: if cond is None: print(&#39;You have to specify cond, for example like so: cond = (df_1.Fake_Year&lt;1999) | (df_1.Fake_Month&lt;6)&#39;) else: train_idx = np.where( cond)[0] test_idx = np.where(~cond)[0] X_train = df_1.iloc[train_idx] y_train = X_train[dep_var] X_train = X_train[cols] X_test = df_1.iloc[test_idx] y_test = X_test[dep_var] X_test = X_test[cols] else: print(&#39;Something is not working right, did you specify the split_mode?&#39;) return X_train, X_test, y_train, y_test . Let&#39;s check our function for all three split_modes: . X_train, X_test, y_train, y_test = split_df(df=df_1, x_cols=cols, dep_var=dep_var, test_size=0.33, split_mode=&#39;random&#39;) X_train.shape, X_test.shape, y_train.shape, y_test.shape . ((978, 35), (482, 35), (978,), (482,)) . X_train, X_test, y_train, y_test = split_df(df=df_1, x_cols=cols, dep_var=dep_var, test_size=0.33, split_mode=&#39;on_split_id&#39;, split_var=split_var) X_train.shape, X_test.shape, y_train.shape, y_test.shape . ((1022, 35), (438, 35), (1022,), (438,)) . X_train, X_test, y_train, y_test = split_df(df=df_1, x_cols=cols, dep_var=dep_var, test_size=0.33, split_mode=&#39;on_condition&#39;, cond=cond) X_train.shape, X_test.shape, y_train.shape, y_test.shape . ((1413, 35), (47, 35), (1413,), (47,)) . Absolutely brilliant. And we&#39;re almost done now. The last piece of the puzzle is the transformation and standardization of our data. We convert the categorical variables into numbers and save the mapping into a dictionary. We standardize the continuous variables with a standardizer of choice and save it. IMPORTANT: we build the dictionaries and the standardizer on the trainset only and use it on the testset to avoid any spillover effects. . First, we build a function which will transform all of our categorical variables and save it to path. We therefore need a function to save our dicts (and later load them). . def cat_transform(X_train, X_test, cat_type, path=&#39;&#39;): dict_list = [] dict_inv_list = [] for i in cat_type: dict_ = dict( enumerate(X_train[i].cat.categories ) ) dict_inv_ = {v: k for k, v in dict_.items()} X_train[i] = X_train[i].map(dict_inv_) X_test[i] = X_test[i].map(dict_inv_) dict_list.append(dict_) dict_inv_list.append(dict_inv_list) dict_name = f&#39;{path}dict_list_cat&#39; save_obj(dict_list, dict_name) dict_inv_name = f&#39;{path}dict_inv_list_cat&#39; save_obj(dict_inv_list, dict_inv_name) return X_train, X_test, dict_list, dict_inv_list . X_train, X_test, y_train, y_test = split_df(df=df_1, x_cols=cols, dep_var=dep_var, test_size=0.33, split_mode=&#39;random&#39;) X_train, X_test, dict_list, dict_inv_list = cat_transform(X_train, X_test, categorical_type) . X_train[categorical_type].head() . MSZoning Street OverallQual SaleType CentralAir . 744 3 | 1 | 7 | 8 | 1 | . 492 3 | 1 | 5 | 6 | 1 | . 307 4 | 1 | 5 | 8 | 1 | . 985 3 | 1 | 4 | 3 | 0 | . 484 3 | 1 | 4 | 8 | 1 | . With this function we automatically saved our dictionaries and mappings to the corresponding path. Next, we take the continuous variables, use a standardizer of our choice and save the standardizer to path. We have to indicate the ID column, because we do not want to standardize this column. We also have to define whether we want to standardize our target variable. . def cont_standardize(X_train, X_test, y_train, y_test, cat_type=None, id_type=None, transform_y=True, path=&#39;&#39;, standardizer=&#39;StandardScaler&#39;): if standardizer ==&#39;StandardScaler&#39;: scaler = StandardScaler() if cat_type==None: cont_type = list(X_train.columns) cont_type.remove(id_type) elif id_type==None: list(set(X_train.columns) - set(id_type)) elif cat_type==None and id_type==None: cont_type = list(X_train.columns) else: cont_type = list(set(X_train.columns) - set(cat_type)) cont_type.remove(id_type) X_train[cont_type] = scaler.fit_transform(X_train[cont_type]) X_test[cont_type] = scaler.transform(X_test[cont_type]) scaler_name = f&#39;{path}StandardScaler&#39; save_obj(scaler, scaler_name) if transform_y: scaler_y = StandardScaler() y_train = scaler_y.fit_transform(y_train.values.reshape(-1, 1)) y_test = scaler_y.transform(y_test.values.reshape(-1, 1)) scaler_y_name = f&#39;{path}StandardScaler_y&#39; save_obj(scaler_y, scaler_name) else: pass if transform_y: return X_train, X_test, y_train, y_test, scaler, scaler_y else: return X_train, X_test, y_train, y_test, scaler elif standardizer ==&#39;MinMaxScaler&#39;: scaler = MinMaxScaler() if cat_type==None: cont_type = list(X_train.columns) cont_type.remove(id_type) elif id_type==None: list(set(X_train.columns) - set(id_type)) elif cat_type==None and id_type==None: cont_type = list(X_train.columns) else: cont_type = list(set(X_train.columns) - set(cat_type)) cont_type.remove(id_type) X_train[cont_type] = scaler.fit_transform(X_train[cont_type]) X_test[cont_type] = scaler.transform(X_test[cont_type]) scaler_name = f&#39;{path}MinMaxScaler&#39; save_obj(scaler, scaler_name) if transform_y: scaler_y = MinMaxScaler() y_train = scaler_y.fit_transform(y_train.values.reshape(-1, 1)) y_test = scaler_y.transform(y_test.values.reshape(-1, 1)) scaler_y_name = f&#39;{path}MinMaxScaler_y&#39; save_obj(scaler_y, scaler_name) else: pass if transform_y: return X_train, X_test, y_train, y_test, scaler, scaler_y else: return X_train, X_test, y_train, y_test, scaler else: print(&#39;standardizer can either be StandardScaler or MinMaxScaler&#39;) . id_type=&#39;Id&#39; . X_train, X_test, y_train, y_test, scaler, scaler_y = cont_standardize(X_train, X_test, y_train, y_test, cat_type=categorical_type, id_type=&#39;Id&#39;, transform_y=True) . X_train.head() . Id MSZoning LotFrontage Fake_date_2Week Fake_Week LotArea Street OverallQual YearBuilt CentralAir ... Fake_date_2Day Fake_date_2Dayofweek Fake_date_2Dayofyear Fake_date_2Is_month_end Fake_date_2Is_month_start Fake_date_2Is_quarter_end Fake_date_2Is_quarter_start Fake_date_2Is_year_end Fake_date_2Is_year_start Fake_date_2Elapsed . 744 0.043062 | 3 | -1.147645 | 1.484951 | 1.160387 | -0.511627 | 1 | 7 | 0.716393 | 1 | ... | -0.455404 | 1.448463 | 1.544753 | -0.15861 | -0.186871 | -0.101639 | -0.111456 | 0.0 | -0.064084 | -1.213687 | . 492 -0.554919 | 3 | 1.391675 | -0.384396 | -0.232891 | 0.521756 | 1 | 5 | 1.155090 | 1 | ... | 0.557602 | -0.500088 | -0.401376 | -0.15861 | -0.186871 | -0.101639 | -0.111456 | 0.0 | -0.064084 | -1.640805 | . 307 -0.993913 | 4 | NaN | -1.252306 | -1.029050 | -0.255387 | 1 | 5 | -1.747058 | 1 | ... | 0.670158 | 1.448463 | -1.250422 | -0.15861 | -0.186871 | -0.101639 | -0.111456 | 0.0 | -0.064084 | -0.513396 | . 985 0.614941 | 3 | -0.076369 | -0.117346 | 0.032495 | 0.044997 | 1 | 4 | -0.734681 | 0 | ... | 0.895271 | 1.448463 | -0.086561 | -0.15861 | -0.186871 | -0.101639 | -0.111456 | 0.0 | -0.064084 | 0.043000 | . 484 -0.573903 | 3 | NaN | 0.149703 | -0.630971 | -0.271827 | 1 | 4 | -0.329730 | 1 | ... | 0.107377 | -1.474364 | 0.132856 | -0.15861 | -0.186871 | -0.101639 | -0.111456 | 0.0 | -0.064084 | 0.887644 | . 5 rows × 35 columns . y_train[:5] . array([[-0.02988161], [-0.11953991], [-1.1544938 ], [-0.71334758], [-0.62014767]]) . We have now saved our StandardScaler to path and transformed the data the way we wanted it. To see the &quot;real&quot; values of our y_train we simply invert our standardizer: . scaler_y.inverse_transform(y_train[:5]) . array([[180000.], [172785.], [ 89500.], [125000.], [132500.]]) . Beautiful! And now I want to show you how incredibly quickly we can now setup our data. I deleted everything in memory and started from here. . Complete run-through . df = pd.read_csv(&#39;train.csv&#39;) to_keep = [&#39;Id&#39;, &#39;MSZoning&#39;, &#39;LotFrontage&#39;, &#39;LotArea&#39;, &#39;Street&#39;, &#39;OverallQual&#39;, &#39;YearBuilt&#39;, &#39;CentralAir&#39;, &#39;SaleType&#39;, &#39;SalePrice&#39;] df = df[to_keep] df[&quot;Fake_date&quot;] = np.random.choice(pd.date_range(&#39;1980-01-01&#39;, &#39;2000-01-01&#39;), len(df)).astype(&#39;str&#39;) df.head() . Id MSZoning LotFrontage LotArea Street OverallQual YearBuilt CentralAir SaleType SalePrice Fake_date . 0 1 | RL | 65.0 | 8450 | Pave | 7 | 2003 | Y | WD | 208500 | 1988-10-01T00:00:00.000000000 | . 1 2 | RL | 80.0 | 9600 | Pave | 6 | 1976 | Y | WD | 181500 | 1989-05-17T00:00:00.000000000 | . 2 3 | RL | 68.0 | 11250 | Pave | 7 | 2001 | Y | WD | 223500 | 1992-09-20T00:00:00.000000000 | . 3 4 | RL | 60.0 | 9550 | Pave | 7 | 1915 | Y | WD | 140000 | 1984-08-11T00:00:00.000000000 | . 4 5 | RL | 84.0 | 14260 | Pave | 8 | 2000 | Y | WD | 250000 | 1986-04-07T00:00:00.000000000 | . date_type = [&#39;Fake_date&#39;] continuous_type = [&#39;LotFrontage&#39;, &#39;LotArea&#39;, &#39;YearBuilt&#39;, &#39;SalePrice&#39;] categorical_type = [&#39;MSZoning&#39;, &#39;Street&#39;, &#39;OverallQual&#39;, &#39;SaleType&#39;,&#39;CentralAir&#39;] . df_1 = df_to_type(df, date_type, continuous_type, categorical_type) . dep_var = &#39;SalePrice&#39; cols = list(df_1.columns) cols.remove(dep_var) . X_train, X_test, y_train, y_test = split_df(df=df_1, x_cols=cols, dep_var=dep_var, test_size=0.33, split_mode=&#39;random&#39;) . X_train, X_test, dict_list, dict_inv_list = cat_transform(X_train, X_test, categorical_type) . X_train, X_test, y_train, y_test, scaler, scaler_y = cont_standardize(X_train, X_test, y_train, y_test, cat_type=categorical_type, id_type=&#39;Id&#39;, transform_y=True) . X_train.head() . Id MSZoning LotFrontage Fake_Week LotArea Street OverallQual YearBuilt CentralAir SaleType ... Fake_Day Fake_Dayofweek Fake_Dayofyear Fake_Is_month_end Fake_Is_month_start Fake_Is_quarter_end Fake_Is_quarter_start Fake_Is_year_end Fake_Is_year_start Fake_Elapsed . 253 254 | 3 | 0.570755 | -0.515555 | -0.113494 | 1 | 5 | -0.242016 | 1 | 1 | ... | -1.189170 | -1.484321 | -0.556901 | -0.171679 | -0.180928 | -0.111456 | -0.106655 | -0.064084 | -0.045268 | -1.648408 | . 1185 1186 | 3 | -0.416423 | 0.870733 | -0.079594 | 1 | 4 | -1.588959 | 1 | 8 | ... | -0.623669 | 1.480279 | 0.922893 | -0.171679 | -0.180928 | -0.111456 | -0.106655 | -0.064084 | -0.045268 | -1.231187 | . 515 516 | 3 | 0.926139 | 0.540664 | 0.137262 | 1 | 9 | 1.273295 | 1 | 6 | ... | 1.525232 | -0.496121 | 0.536450 | -0.171679 | -0.180928 | -0.111456 | -0.106655 | -0.064084 | -0.045268 | -0.907465 | . 433 434 | 3 | 1.163061 | 0.012555 | 0.016602 | 1 | 5 | 0.869212 | 1 | 8 | ... | -0.962969 | 0.986179 | 0.036902 | -0.171679 | -0.180928 | -0.111456 | -0.106655 | -0.064084 | -0.045268 | -0.932367 | . 1406 1407 | 3 | -0.021552 | -0.911637 | -0.192566 | 1 | 4 | 0.027373 | 1 | 8 | ... | 1.412132 | 0.986179 | -0.924493 | -0.171679 | -0.180928 | -0.111456 | -0.106655 | -0.064084 | -0.045268 | 1.422486 | . 5 rows × 22 columns . y_train[:5] . array([[-0.2703821 ], [-0.96272526], [ 2.92223258], [ 0.02950289], [-0.59634404]]) . Absolutely brilliant. Look how easy and fast we pre-processed our data. We now can start any Machine Learning Algorithm we want. It&#39;s super easy to replicate the results, because we saved the mapping and the scaler. We automatically created train and testset, we are flexible to use this in any way we want to split our data. I hope this will come in handy for your next machine learning project. . Stay tuned for the next blogpost! . Lasse .",
            "url": "https://lschmiddey.github.io/fastpages_/2020/10/22/How-to-preprocess-data-for-machine-learning.html",
            "relUrl": "/2020/10/22/How-to-preprocess-data-for-machine-learning.html",
            "date": " • Oct 22, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "How to deploy a machine learninig model",
            "content": "In this blogpost I will show you how to first create a simple Random Forest Classifier and then build an API with Flask. Let&#39;s start by building the model. . import numpy as np import pandas as pd from pathlib import Path import pickle from sklearn.ensemble import RandomForestClassifier from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split . p = Path(&#39;/notebooks/storage/data/Titanic&#39;) df = pd.read_csv(f&#39;{p}/train/train.csv&#39;, index_col=0) df.tail(5) . Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . PassengerId . 887 0 | 2 | Montvila, Rev. Juozas | male | 27.0 | 0 | 0 | 211536 | 13.00 | NaN | S | . 888 1 | 1 | Graham, Miss. Margaret Edith | female | 19.0 | 0 | 0 | 112053 | 30.00 | B42 | S | . 889 0 | 3 | Johnston, Miss. Catherine Helen &quot;Carrie&quot; | female | NaN | 1 | 2 | W./C. 6607 | 23.45 | NaN | S | . 890 1 | 1 | Behr, Mr. Karl Howell | male | 26.0 | 0 | 0 | 111369 | 30.00 | C148 | C | . 891 0 | 3 | Dooley, Mr. Patrick | male | 32.0 | 0 | 0 | 370376 | 7.75 | NaN | Q | . What we want to predict is whether the Passenger has survived or not. We want a simple model, so let&#39;s only keep the following variables: . to_keep = [&#39;Survived&#39;, &#39;Pclass&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;Parch&#39;] final_df = df.reset_index()[to_keep] . final_df.dtypes . Survived int64 Pclass int64 Sex object Age float64 Parch int64 dtype: object . final_df[&#39;Sex&#39;] = np.where(final_df[&#39;Sex&#39;]==&#39;male&#39;, 0, 1) . final_df.head() . Survived Pclass Sex Age Parch . 0 0 | 3 | 0 | 22.0 | 0 | . 1 1 | 1 | 1 | 38.0 | 0 | . 2 1 | 3 | 1 | 26.0 | 0 | . 3 1 | 1 | 1 | 35.0 | 0 | . 4 0 | 3 | 0 | 35.0 | 0 | . Now that we&#39;ve created our final dataframe, let&#39;s train the model. . First, we define the accuracy metric to see how our model is doing: . def accu(pred,y, threshold=0.5): return (np.round(pred-threshold+0.5) == y).mean() def m_accu(m, xs, y): return np.round(accu(m.predict(xs), y), 3) . We then standardize our data, in this case only the age variable. . final_df = final_df.fillna(0) . scaler = StandardScaler() final_df[&#39;Age&#39;] = scaler.fit_transform(final_df[&#39;Age&#39;].values.reshape(-1, 1)) . Then we define the model: . def rf(xs, y, n_estimators=40, max_samples=500, max_features=0.5, min_samples_leaf=5, **kwargs): return RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators, max_samples=max_samples, max_features=max_features, min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y) . Then we split the data into train and validation. . col_names = [&#39;Pclass&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;Parch&#39;] dep_var = &#39;Survived&#39; . xs, xs_valid, y, y_valid = train_test_split(final_df[col_names], final_df[dep_var], test_size=0.33, random_state=42) . And finally let&#39;s train the model: . m = rf(xs, y); . m_accu(m, xs, y), m_accu(m, xs_valid, y_valid) . (0.836, 0.82) . from sklearn.metrics import confusion_matrix confusion_matrix(y_valid, np.round(m.predict(xs_valid))) . array([[165, 10], [ 43, 77]]) . I guess that looks ok, so I will save the final model together with the standardizer. . def save_obj(obj, name ): with open(f&#39;{name}.pkl&#39;, &#39;wb&#39;) as f: pickle.dump(obj, f) def load_obj(name ): with open(f&#39;{name}.pkl&#39;, &#39;rb&#39;) as f: return pickle.load(f) . filename = &#39;Flask_App/app/final_model.pkl&#39; pickle.dump(m, open(filename, &#39;wb&#39;)) save_obj(scaler, &#39;Flask_App/app/standardizer&#39;) . Create an API with Flask . Now that we have our model we want to build an API. We will use Flask for this. This is how the folder structure should look like: . . For our Flask App to run we need our terminal. Here are the bash commands you need to run to set up Flask: . python3 -m venv venv . venv/bin/activate pip install Flask pip install pandas numpy sklearn requests pip freeze &gt; requirements.txt cd app export FLASK_APP=main.py export FLASK_ENV=development flask run . In our folder we create a virtual environment which I called venv. I then activate it an install Flask and some other packages we need to make our RF-Model work. I then save the requirements into a txt file. I go into the app folder, set the Flask App to main.py and the environment to development. Finally, I start flask. The output should look like this: . . Next, we need to define a main.py and a utils.py. Let&#39;s start with the utils.py . With our API we want to send a GET request with values for our variables: . Pclass = 1 Age = 22.0 Sex = 0 Parch = 1 # load trained model and standardizer PATH = &quot;final_model.pkl&quot; loaded_model = pickle.load(open(PATH, &#39;rb&#39;)) standardizer = load_obj(&#39;standardizer&#39;) # create dataframe from input data df = pd.DataFrame({&#39;Pclass&#39;: [Pclass], &#39;Age&#39;: [Age], &#39;Sex&#39;:[Sex], &#39;Parch&#39;:[Parch]}) # define transform function def transform_data(raw_data): raw_data = raw_data.fillna(0) raw_data[&#39;Age&#39;] = standardizer.transform(raw_data[&#39;Age&#39;].values.reshape(-1, 1)) return raw_data # define prediction function def get_prediction(transformed_data): pred = loaded_model.predict_proba(transformed_data) return pred . Does it work? . get_prediction(transform_data(df)) . array([[0.65921623, 0.34078377]]) . Awesome! So our utils.py works, let&#39;s build our main.py. . from flask import Flask, request, jsonify import csv import pandas as pd from utils import transform_data, get_prediction app = Flask(__name__) @app.route(&#39;/predict&#39;, methods=[&quot;GET&quot;]) def predict(): if request.method == &#39;GET&#39;: Pclass = request.args.get(&#39;Pclass&#39;) Age = request.args.get(&#39;Age&#39;) Sex = request.args.get(&#39;Sex&#39;) Parch = request.args.get(&#39;Parch&#39;) df = pd.DataFrame({&#39;Pclass&#39;: [Pclass], &#39;Age&#39;: [Age], &#39;Sex&#39;:[Sex], &#39;Parch&#39;:[Parch]}) transf_data = transform_data(raw_data) prediction = get_prediction(transf_data) prediction = prediction[0][1].item() # We take the first value of our predictions, representing the probability to survive. data = {&#39;prediction&#39;: prediction} return jsonify(data) else: return jsonify({&#39;error&#39;: &#39;Only GET requests possible&#39;}) . What this does is the following: it reads the data from the GET request, uses the utils.py for transform_data and get_predictions and returns the prediction for surviving. . Finally, we want to check our app. We build a test.py file where we send data to our &quot;server&quot;, which will return a result. This is how the test.py file looks like: . import requests # https://your-heroku-app-name.herokuapp.com/predict # http://localhost:5000/predict data = {&#39;Pclass&#39;: 1, &#39;Age&#39;: 22.0, &#39;Sex&#39;: 0, &#39;Parch&#39;: 1} r = requests.get(&quot;http://localhost:5000/predict&quot;, params=data) . Check our API: . Let&#39;s use our terminal to see whether our API is working. . . Awesome! We built a Flask App to deploy our machine learning model and made it available as an API. . Stay tuned for the next blogpost! Lasse .",
            "url": "https://lschmiddey.github.io/fastpages_/2020/10/19/Deploy_Machine-Learning-Model.html",
            "relUrl": "/2020/10/19/Deploy_Machine-Learning-Model.html",
            "date": " • Oct 19, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "How to use fastai tabular with custom metric",
            "content": "How to easily train a tabular model with fastai . First, we have to load our data (I used the kaggle API to do this, check out my previous blogpost) . from pathlib import Path p = Path(&#39;/notebooks/storage/data/Titanic&#39;) filename = Path(&#39;/notebooks/storage/data/Titanic/titanic.zip&#39;) . !unzip -q {str(filename)} -d {str(p/&quot;train&quot;)} . Get imports . import pandas as pd from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype from fastai.tabular.all import * from sklearn.metrics import confusion_matrix . df = pd.read_csv(f&#39;{p}/train/train.csv&#39;, index_col=0) df.tail(5) . Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . PassengerId . 887 0 | 2 | Montvila, Rev. Juozas | male | 27.0 | 0 | 0 | 211536 | 13.00 | NaN | S | . 888 1 | 1 | Graham, Miss. Margaret Edith | female | 19.0 | 0 | 0 | 112053 | 30.00 | B42 | S | . 889 0 | 3 | Johnston, Miss. Catherine Helen &quot;Carrie&quot; | female | NaN | 1 | 2 | W./C. 6607 | 23.45 | NaN | S | . 890 1 | 1 | Behr, Mr. Karl Howell | male | 26.0 | 0 | 0 | 111369 | 30.00 | C148 | C | . 891 0 | 3 | Dooley, Mr. Patrick | male | 32.0 | 0 | 0 | 370376 | 7.75 | NaN | Q | . df.shape . (891, 11) . Let&#39;s check out the data type for each column. . df.dtypes . Survived int64 Pclass int64 Name object Sex object Age float64 SibSp int64 Parch int64 Ticket object Fare float64 Cabin object Embarked object dtype: object . We see that &quot;Pclass&quot;, &quot;Sex&quot;, &quot;Cabin&quot; and &quot;Embarked&quot; are objects. When we think about these variables, they clearly are categorical variables. So let&#39;s change that. Also, this blogpost is not about how to train the best possible model, so we do not want any fancy feature engineering. I therefore will get rid of the variable &quot;Name&quot;. . df[&#39;Pclass&#39;] = df[&#39;Pclass&#39;].astype(&#39;category&#39;) df[&#39;Sex&#39;] = df[&#39;Sex&#39;].astype(&#39;category&#39;) df[&#39;Cabin&#39;] = df[&#39;Cabin&#39;].astype(&#39;category&#39;) df[&#39;Embarked&#39;] = df[&#39;Embarked&#39;].astype(&#39;category&#39;) . df = df.drop(columns=[&#39;Name&#39;]) . I want to build a validation set, based on the index. The easiest way is to create an array with the length of our data, randomly select a percentage of indeces we want in our training data, and use the leftover indeces for our validation set. . # Define percentage of train data, define length of training indeces and validation indeces p = 0.9 len_df = len(df) len_idx_tain = round(len_df*p) len_idx_val = len_df-len_idx_tain . # build array of indeces for training and validation idx_arr = range(0,len_df) train_idx = np.random.choice(range(0,len_df), len_idx_tain, replace=False) val_idx = [i for i in idx_arr if i not in train_idx] val_idx = np.asarray(val_idx) . Let&#39;s put these arrays into a split variable, which fastai knows how to make use of to split the data into training data and validation data. . splits = (list(train_idx),list(val_idx)) . Then we make use of the &quot;cont_cat_split&quot; function to easily get a list of column names for each categorical and continuous variable. . cont_nn, cat_nn = cont_cat_split(df, dep_var=&#39;Survived&#39;) . cont_nn, cat_nn . ([&#39;Age&#39;, &#39;Fare&#39;], [&#39;Pclass&#39;, &#39;Sex&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Ticket&#39;, &#39;Cabin&#39;, &#39;Embarked&#39;]) . That looks great! How about our missing values? . df.isna().sum() . Survived 0 Pclass 0 Sex 0 Age 177 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 dtype: int64 . Looks like there are quite a few missing values for age and cabin. But we know a way to make use of that! Even &quot;no-information&quot; can be an information. . procs_nn = [Categorify, FillMissing, Normalize] . Fastai provides us with many super useful out-of-the-box functions. Three of them you can see here: Categorify will take all of the variables with type &quot;category&quot; will replace the category with a numerical number and write the mapping into a dictionary. FillMissing, well, does exactly this. It also provides us with a extra boolean variable, indicating whether the row has a missing value for a specific variable (e.g. age). Normalize then normalizes our continuous data and saves the standardzier. . df[&#39;Survived&#39;] = df[&#39;Survived&#39;].astype(np.float32) . This one is important: fastai needs the dependend variable to be of type float32, even though we have a binary classification! We need this, because the loss function will be mse-loss. This leads to possibly odd behavior: the numbers can get below 0 and even below -1 or above 2! If we use a provided metric like accuracy, this leads to odd results. So we will write our own custom metric! But first, let&#39;s use the TabularPandas class and put that in a dataloader (important: do not let the batchsize become bigger than your data, this will result in a error!). . to_nn = TabularPandas(df, procs_nn, cat_nn, cont_nn, splits=splits, y_names=&#39;Survived&#39;) # length of data is 891, so let the batchsize be smaller than that! dls = to_nn.dataloaders(256) . Train Model . Now is the time to train our model. But wait! We first have to define a custom metric, because the accuracy provided by fastai will lead to odd results. I can show you what I mean: . learn = tabular_learner(dls, layers=[64, 8], n_out=1, metrics=[accuracy]) . learn.fit_one_cycle(4,1e-3) . epoch train_loss valid_loss accuracy time . 0 | 0.562715 | 0.504298 | 0.617977 | 00:00 | . 1 | 0.498489 | 0.477128 | 0.617977 | 00:00 | . 2 | 0.436740 | 0.448243 | 0.617977 | 00:00 | . 3 | 0.391733 | 0.423635 | 0.617977 | 00:00 | . What is going on here? Is the accuracy not improving? The answer is no. Tabular learner in combination with accuracy does not give us the result we want. Why is that? First, our predictions will not be 0 or 1, but floating point values. That&#39;s good for our loss, so our model can learn from its gradient (checkout my blogpost about gradients if you do not know what that means). But this leads to a malfunctioning of the accuracy metric, which fastai uses from sklearn. What we need to do is to define our own custom metric. Let&#39;s do that! . import sklearn.metrics as skm def _accumulate(self, learn): #pred = learn.pred.argmax(dim=self.dim_argmax) if self.dim_argmax else learn.pred m = nn.Sigmoid() pred = learn.pred pred = torch.round(m(pred)) targ = learn.y pred,targ = to_detach(pred),to_detach(targ) self.preds.append(pred) self.targs.append(targ) AccumMetric.accumulate = _accumulate def BinAccu(): return skm_to_fastai(skm.accuracy_score) . So what&#39;s going on here? Next to Callbacks, fastai provides a clever way how to customize metrics. To be honest, it took me some time to figure it out, but here&#39;s how it works. Each learner has a .pred and a .y, which means this is how you can get its predictions and its targets. As I already mentioned, preds are floating point values, but we want them to be between 0 and 1 - luckily, this is exactly what the sigmoid function is doing. Then we want the values to be either 0 or 1, so let&#39;s just round these values (threshold here is .5, but you can fiddle around with that). We then append these &quot;new&quot; preds and targets. . &quot;In order to provide a more flexible foundation to support metrics like this fastai provides a Metric abstract class which defines three methods: reset, accumulate, and value (which is a property). Reset is called at the start of training, accumulate is called after each batch, and then finally value is called to calculate the final check.&quot; . So with this function we directly sit on top of accumulate. The function skm_to_fastai let&#39;s you use sklearn metrics (in this case: accuracy_score) and uses the pred and targ we provided in our tiny function. Important: we have to instanciate the instance first! . binaccu = BinAccu() learn = tabular_learner(dls, n_out=1, metrics=[binaccu]) . Ok, we&#39;re good to go. Let&#39;s use fastai&#39;s awesome lr_find(). . learn.lr_find() . SuggestedLRs(lr_min=0.03630780577659607, lr_steep=0.0010000000474974513) . Looks like we should set our learning rate to about 1e-3. . learn.fit_one_cycle(10,1e-3) . epoch train_loss valid_loss accuracy_score time . 0 | 0.484679 | 0.372455 | 0.325843 | 00:00 | . 1 | 0.425642 | 0.374143 | 0.595506 | 00:00 | . 2 | 0.343880 | 0.367273 | 0.494382 | 00:00 | . 3 | 0.271155 | 0.349920 | 0.393258 | 00:00 | . 4 | 0.220030 | 0.334517 | 0.382022 | 00:00 | . 5 | 0.184440 | 0.325883 | 0.382022 | 00:00 | . 6 | 0.157990 | 0.319556 | 0.382022 | 00:00 | . 7 | 0.137255 | 0.313525 | 0.382022 | 00:00 | . 8 | 0.120796 | 0.309076 | 0.382022 | 00:00 | . 9 | 0.107582 | 0.306098 | 0.382022 | 00:00 | . Our accuracy is improving until epoch 2. So we should restart our training. But first, let&#39;s have a look, whether our accuracy score is doing what we expect: . preds, targs = learn.get_preds() m = nn.Sigmoid() confusion_matrix(torch.round(m(preds.view(-1))).numpy(), targs.view(-1).numpy()) . array([[ 0, 0], [55, 34]]) . 1-abs(torch.round(m(preds.view(-1))) - targs.view(-1)).sum()/len((torch.round(m(preds.view(-1))) - targs.view(-1))) . tensor(0.3820) . Awesome. That&#39;s exactly what we wanted (not the result though, which is pretty bad ;) ) . Let&#39;s re-run the trainig. . binaccu = BinAccu() learn = tabular_learner(dls, layers=[128,128], n_out=1, metrics=[binaccu]) . learn.fit_one_cycle(5,1e-5) . epoch train_loss valid_loss accuracy_score time . 0 | 0.616267 | 0.422966 | 0.617978 | 00:00 | . 1 | 0.610159 | 0.418977 | 0.617978 | 00:00 | . 2 | 0.607095 | 0.416566 | 0.617978 | 00:00 | . 3 | 0.604457 | 0.414736 | 0.629213 | 00:00 | . 4 | 0.604596 | 0.413118 | 0.640449 | 00:00 | . preds, targs = learn.get_preds() m = nn.Sigmoid() confusion_matrix(torch.round(m(preds.view(-1))).numpy(), targs.view(-1).numpy()) . array([[54, 31], [ 1, 3]]) . To be honest, the results aren&#39;t quite that good. With a RandomForest you can easily get up to 85% accuracy. However, in this blogpost I wanted to show you how to leverage fastai&#39;s tabular_learner and add a custom metric to it. . I hope you stay tuned for the next blogpost! . Lasse .",
            "url": "https://lschmiddey.github.io/fastpages_/2020/10/01/Tabular-Data-with-custom-metric.html",
            "relUrl": "/2020/10/01/Tabular-Data-with-custom-metric.html",
            "date": " • Oct 1, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Build a web app with binder",
            "content": "Let&#39;s deploy the model! . This is the last part of my small project to get an app running, which will take an image as input, extract the text and then run a deep learning model on it to give the book a rating based on my book taste. In part 3 we put together the parts we need to create our web app with binder and voila. . The final project you&#39;ll find in my git repo. This blogpost should guide you through the git repo, what to do to build your web app from a jupyter notebook. Ok, so let&#39;s get started! . . Alright, what have we got here? First of all, the jupyter notebook from part 3. So nothing new here. We have a LICENSE and a README file, which aren&#39;t that important. What&#39;s next is important, the three files &quot;app.yml&quot;, &quot;apt.txt&quot; and &quot;requirements.txt&quot;. Let&#39;s start with &quot;requirements.txt&quot;. . . In the &quot;requirements.txt&quot; are all the packages we need for our project. Binder will create a docker container for us, so it needs to know what packages are required. Next, let&#39;s have a look at &quot;apt.txt&quot;. . . This one is moderatly more difficult. Especially if you do haven&#39;t worked with docker before. It looks like the requirement file, however what this does is it apt installs these packages. This is required, because pytesseract works different from other packages. Without this file our jupyter notebook won&#39;t be able to connect to pytesseract. I had a lot of head scratching and googling to do before I found out that tesseract needs to be installed that way to find the path where its libraries are. . Next, let&#39;s look at app.yml. . . This file is needed to that we can make use of voila. Voila takes all the non html Output, runs it in the background and doesn&#39;t show it to the user of the app. . Ok, we&#39;re finally able to start our web app on Binder. . . So we simply specify the path to our git repository and type &quot;/voila/render/App-with-Voila-mybinder.ipynb&quot; into the URL to open. Then instead of &quot;File&quot; we use &quot;URL&quot;. . Ok, let Binder do its work and set up a docker container for us, on which our app is running. When Binder creates the docker container for the project for the first time, this will take a while (5 min or so). . Let&#39;s have a look at the running app. . . Let&#39;s uploade an image and see what my algorithm will say how much I will like this book: . . Awesome! That looks great (btw. the book was the sea wolf by Jack London and I loved this book). Now everyone can use this little web app and see whether I would recommend this book or not. Simply click on the &quot;launch binder&quot; in my git repo and start the app. I hope you enjoyed this little project as much as I did and stay tuned for more. . Lasse .",
            "url": "https://lschmiddey.github.io/fastpages_/2020/09/28/Build-binder-app-Part4.html",
            "relUrl": "/2020/09/28/Build-binder-app-Part4.html",
            "date": " • Sep 28, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Lasse's book recommender",
            "content": "How would Lasse rate this book? . This is the third part of my little project to build a rating system on text which we extract from images and which in turn leads to a rating on how much I will like this book. In this notebook I want to show you how to make use of ipywidgets to make a notebook which we can use as a web appplication. Furthermore, I will show you how to download the trained model from part 2 from my private GoogleDrive. So let&#39;s get started! . !pip install googledrivedownloader !pip install transformers . Collecting googledrivedownloader Downloading googledrivedownloader-0.4-py2.py3-none-any.whl (3.9 kB) Installing collected packages: googledrivedownloader Successfully installed googledrivedownloader-0.4 Collecting transformers Downloading transformers-3.3.0-py3-none-any.whl (1.1 MB) |████████████████████████████████| 1.1 MB 14.9 MB/s eta 0:00:01 |██▉ | 92 kB 15.2 MB/s eta 0:00:01 |███████████████████████████████ | 1.0 MB 14.9 MB/s eta 0:00:01 Requirement already satisfied: tqdm&gt;=4.27 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from transformers) (4.48.2) Requirement already satisfied: requests in /opt/conda/envs/fastai/lib/python3.8/site-packages (from transformers) (2.24.0) Requirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from transformers) (0.1.86) Requirement already satisfied: numpy in /opt/conda/envs/fastai/lib/python3.8/site-packages (from transformers) (1.19.1) Collecting filelock Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB) Collecting regex!=2019.12.17 Downloading regex-2020.9.27-cp38-cp38-manylinux2010_x86_64.whl (675 kB) |████████████████████████████████| 675 kB 7.2 MB/s eta 0:00:01 Collecting sacremoses Downloading sacremoses-0.0.43.tar.gz (883 kB) |████████████████████████████████| 883 kB 25.0 MB/s eta 0:00:01 Requirement already satisfied: packaging in /opt/conda/envs/fastai/lib/python3.8/site-packages (from transformers) (20.4) Collecting tokenizers==0.8.1.rc2 Downloading tokenizers-0.8.1rc2-cp38-cp38-manylinux1_x86_64.whl (3.0 MB) |████████████████████████████████| 3.0 MB 23.4 MB/s eta 0:00:01 Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from requests-&gt;transformers) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from requests-&gt;transformers) (2020.6.20) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from requests-&gt;transformers) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from requests-&gt;transformers) (1.25.10) Requirement already satisfied: six in /opt/conda/envs/fastai/lib/python3.8/site-packages (from sacremoses-&gt;transformers) (1.15.0) Collecting click Downloading click-7.1.2-py2.py3-none-any.whl (82 kB) |████████████████████████████████| 82 kB 1.3 MB/s eta 0:00:01 Requirement already satisfied: joblib in /opt/conda/envs/fastai/lib/python3.8/site-packages (from sacremoses-&gt;transformers) (0.16.0) Requirement already satisfied: pyparsing&gt;=2.0.2 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from packaging-&gt;transformers) (2.4.7) Building wheels for collected packages: sacremoses Building wheel for sacremoses (setup.py) ... done Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=5a3671aa51ef5e5501f57c816f31eea01e646340384391c47489f75a0c3cb57c Stored in directory: /root/.cache/pip/wheels/7b/78/f4/27d43a65043e1b75dbddaa421b573eddc67e712be4b1c80677 Successfully built sacremoses Installing collected packages: filelock, regex, click, sacremoses, tokenizers, transformers Successfully installed click-7.1.2 filelock-3.0.12 regex-2020.9.27 sacremoses-0.0.43 tokenizers-0.8.1rc2 transformers-3.3.0 . from fastai.vision.all import * from fastai.vision.widgets import * from fastai.vision.widgets import * from PIL import Image, ImageFilter import pytesseract import re from transformers import BertTokenizer, BertForSequenceClassification from pathlib import Path from torch.utils.data import TensorDataset, DataLoader . Lots of models especially in the deep learning context can get quite large. I wasn&#39;t able to upload my model into git, so I thought of a way to get around that. I uploaded my trained model from part 2 into my GoogleDrive and then use the google_drive_downloader to download my model into my notebook. . from google_drive_downloader import GoogleDriveDownloader as gdd gdd.download_file_from_google_drive(file_id=&#39;1kk_SvwpwZeuLnZirW5vbrd8FEnm7yJRt&#39;, dest_path=&#39;./export.pkl&#39;, unzip=True) . Downloading 1kk_SvwpwZeuLnZirW5vbrd8FEnm7yJRt into ./export.pkl... Done. Unzipping...Done. . import warnings warnings.filterwarnings(&quot;ignore&quot;) . Next, we use all the steps you already know from part 2: rotate the image and filter it, use pytesseract to extract the text from the image, tokenize the text and put it in a dataloader and download the pre-trained model from the awesome huggingface library. . def proc_img(input_img): img = input_img.rotate(angle=270, resample=0, expand=10, center=None, translate=None, fillcolor=None) img = img.filter(ImageFilter.MedianFilter) return img . def get_text(img): return pytesseract.image_to_string(img, lang=&quot;deu&quot;) . def use_pattern(text): return pattern.sub(lambda m: rep[re.escape(m.group(0))], text) . rep = {&quot; n&quot;: &quot;&quot;, &quot;`&quot;: &quot;&quot;, &#39;%&#39;:&quot;&quot;, &#39;°&#39;: &#39;&#39;, &#39;&amp;&#39;:&#39;&#39;, &#39;‘&#39;:&#39;&#39;, &#39;€&#39;:&#39;e&#39;, &#39;®&#39;:&#39;&#39;, &#39; &#39;: &#39;&#39;, &#39;5&#39;:&#39;s&#39;, &#39;1&#39;:&#39;i&#39;, &#39;_&#39;:&#39;&#39;, &#39;-&#39;:&#39;&#39;} # define desired replacements here # use these three lines to do the replacement rep = dict((re.escape(k), v) for k, v in rep.items()) #Python 3 renamed dict.iteritems to dict.items so use rep.items() for latest versions pattern = re.compile(&quot;|&quot;.join(rep.keys())) . # Tokenize all of the sentences and map the tokens to thier word IDs. def tokenize_text(sent): input_ids = [] attention_masks = [] encoded_dict = tokenizer.encode_plus( sent, # Sentence to encode. add_special_tokens = True, # Add &#39;[CLS]&#39; and &#39;[SEP]&#39; truncation=True, max_length = 256, # Pad &amp; truncate all sentences. pad_to_max_length = True, #padding=&#39;longest&#39;, return_attention_mask = True, # Construct attn. masks. return_tensors = &#39;pt&#39;, # Return pytorch tensors. ) # Add the encoded sentence to the list. input_ids.append(encoded_dict[&#39;input_ids&#39;]) # And its attention mask (simply differentiates padding from non-padding). attention_masks.append(encoded_dict[&#39;attention_mask&#39;]) # Convert the lists into tensors. input_ids = torch.cat(input_ids, dim=0) attention_masks = torch.cat(attention_masks, dim=0) return input_ids, attention_masks . def create_dataloader(text): input_ids, attention_masks = tokenize_text(text) dataset = TensorDataset(input_ids, attention_masks) batch_size = 1 app_dataloader = DataLoader( dataset, # The validation samples. batch_size = batch_size # Evaluate with this batch size. ) return app_dataloader . def predict(dataloader): # Prediction on test set device = torch.device(&#39;cpu&#39;) # Put model in evaluation mode model.eval() # Tracking variables predictions = [] # Predict for batch in dataloader: # Add batch to CPU batch = tuple(t.to(device) for t in batch) # Unpack the inputs from our dataloader b_input_ids, b_input_mask = batch # Telling the model not to compute or store gradients, saving memory and # speeding up prediction with torch.no_grad(): # Forward pass, calculate logit predictions outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask) logits = outputs[0] # Move logits and labels to CPU logits = logits.detach().cpu().numpy() # Store predictions and true labels predictions.append(logits) return np.argmax(predictions) . PRE_TRAINED_MODEL_NAME = &#39;bert-base-german-cased&#39; # Load the BERT tokenizer tokenizer = torch.hub.load(&#39;huggingface/pytorch-transformers&#39;, &#39;tokenizer&#39;, PRE_TRAINED_MODEL_NAME) # Download vocabulary from S3 and cache. n_classes=5 model = BertForSequenceClassification.from_pretrained( &quot;bert-base-german-cased&quot;, # Use the 12-layer BERT model, with an uncased vocab. num_labels = n_classes, # The number of output labels--2 for binary classification. # You can increase this for multi-class tasks. output_attentions = False, # Whether the model returns attentions weights. output_hidden_states = False, # Whether the model returns all hidden-states. ) . Downloading: &#34;https://github.com/huggingface/pytorch-transformers/archive/master.zip&#34; to /root/.cache/torch/hub/master.zip . . p = Path.cwd() . Even though we trained the model on GPU, that&#39;s not what we want for production. So I load my model onto CPU. . device = torch.device(&#39;cpu&#39;) model.load_state_dict(torch.load(p/&#39;export.pkl&#39;, map_location=device)) . btn_upload = widgets.FileUpload() out_pl = widgets.Output() rating_widget = widgets.Label() btn_run = widgets.Button(description=&#39;Lasses Empfehlung:&#39;) . def on_click_text(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(proc_img(img).to_thumb(256,256)) text = use_pattern(get_text(proc_img(img))) star_rating = predict(create_dataloader(text)) rating_widget.value = f&#39;Lasse würde diesem Buch {star_rating+1} Stern(e) von 5 Sternen geben!&#39; . btn_run.on_click(on_click_text) . VBox([widgets.Label(&#39;Upload Bild von Buchseite&#39;), btn_upload, btn_run, out_pl, rating_widget]) . Perfect, that worked like a charme! Coming up I will show you how to take this notebook and turn it into a little web app. So stay tuned! . Lasse .",
            "url": "https://lschmiddey.github.io/fastpages_/2020/09/27/Prepare-Notebook-for-App-Part3.html",
            "relUrl": "/2020/09/27/Prepare-Notebook-for-App-Part3.html",
            "date": " • Sep 27, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Use pretrained BERT model for classification",
            "content": "Would Lasse recommend this book? . This is the second part of the three part blogpost on my NLP project. In this blogpost I will show you how to use a pretrained BERT model to finetune a model to predict how I would rate a book based on one page. In the first part I showed how to build the dataset. Now I will show you how to use this data to basically build a model from that. First, let&#39;s get our packages. . !pip install transformers !pip install seaborn . from pathlib import Path import numpy as np import pandas as pd import torch import torch.nn as nn from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report import transformers from transformers import BertTokenizer, BertForSequenceClassification # specify GPU device = torch.device(&quot;cuda&quot;) . We then load the data we got from our images in combination with pytesseract. . Load Data . p = Path.cwd() complete_df = pd.read_csv(p/&#39;datasets/text_df.csv&#39;) complete_df.head() . text title rating . 0 war ein schrecklicher Rückfall eingetreten.In ... | gegendenStrich | 1 | . 1 höchst moralischer Akt, die Welt von einem sol... | derSeewolf | 5 | . 2 deutsches Luder nehmen. Und sollten Sie es dan... | ButchersCrossing | 4 | . 3 müssen.»Sie kamen jetzt in die Vorstadt. Die S... | diePest | 2 | . 4 ins Gesicht, wandte sich von ihrem traurigen A... | diePest | 2 | . We only need the text and my rating. We also need to substract 1 from my rating for indexing purposes for the cross entropy loss function. . df = pd.DataFrame({ &#39;label&#39;: complete_df.iloc[:,2]-1, &#39;text&#39;: complete_df.iloc[:,0] }) df.head() . label text . 0 0 | war ein schrecklicher Rückfall eingetreten.In ... | . 1 4 | höchst moralischer Akt, die Welt von einem sol... | . 2 3 | deutsches Luder nehmen. Und sollten Sie es dan... | . 3 1 | müssen.»Sie kamen jetzt in die Vorstadt. Die S... | . 4 1 | ins Gesicht, wandte sich von ihrem traurigen A... | . # Get the lists of sentences and their labels. sentences = df.text.values labels = df.label.values . Bert Tokenizer . My data is in German. Luckily, the awesome huggingface library provides a crazy amount of pretrained models in languages from all over the world. We first need our tokenizer: . PRE_TRAINED_MODEL_NAME = &#39;bert-base-german-cased&#39; # Load the BERT tokenizer tokenizer = torch.hub.load(&#39;huggingface/pytorch-transformers&#39;, &#39;tokenizer&#39;, PRE_TRAINED_MODEL_NAME) # Download vocabulary from S3 and cache. . Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master . Let&#39;s look what the tokenizer does to our text sentences: . # Print the original sentence. print(&#39; Original: &#39;, sentences[0]) # Print the sentence split into tokens. print(&#39;Tokenized: &#39;, tokenizer.tokenize(sentences[0])) # Print the sentence mapped to token ids. print(&#39;Token IDs: &#39;, tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0]))) . Original: war ein schrecklicher Rückfall eingetreten.In dem »verheirateten Priester« wurde das LobChristi von Barbey d’Aurévilly gesungen; in »LesDiaboliques« hatte sich der Verfasser dem Teufel ergeben, den er pries; und jetzt erschien der Sadismus,dieser Bastard des Katholizismus, den die Religion inallen Formen mit Exorzismen und Scheiterhaufendurch alle Jahrhunderte verfolgt hat.Mit Barbey d’Aurévilly nahm die Serie der reli—giösen Schriftsteller ein Ende. Eigentlich gehörte dieser Paria in jeder Hinsicht mehr zur weltlichen Literatur als zu jener andern, bei der er einen Platzbeanspruchte, den man ihm verweigerte. Seine Sprache war die des wilden Romantismus, voll gewunde—ner Wendungen und übertriebener Vergleiche, undeigentlich erschien d’Aurévilly wie ein Zuchthengstunter diesen Wallachen, die die ultramontanen StalleDem Herzog kamen diese Betrachtungen heimgelegentlichen Wiederlesen einiger Stellen diesesC L ( ii i „ .. ‚]:„„„„ „a.—näepn alwxxr9rl’iﬂ» Tokenized: [&#39;war&#39;, &#39;ein&#39;, &#39;schreck&#39;, &#39;##licher&#39;, &#39;Rück&#39;, &#39;##fall&#39;, &#39;eingetreten&#39;, &#39;.&#39;, &#39;In&#39;, &#39;dem&#39;, &#39;[UNK]&#39;, &#39;verheiratet&#39;, &#39;##en&#39;, &#39;Priester&#39;, &#39;[UNK]&#39;, &#39;wurde&#39;, &#39;das&#39;, &#39;Lob&#39;, &#39;##Christ&#39;, &#39;##i&#39;, &#39;von&#39;, &#39;Barb&#39;, &#39;##ey&#39;, &#39;d&#39;, &#39;[UNK]&#39;, &#39;[UNK]&#39;, &#39;gesungen&#39;, &#39;;&#39;, &#39;in&#39;, &#39;[UNK]&#39;, &#39;Les&#39;, &#39;##Di&#39;, &#39;##ab&#39;, &#39;##oli&#39;, &#39;##ques&#39;, &#39;[UNK]&#39;, &#39;hatte&#39;, &#39;sich&#39;, &#39;der&#39;, &#39;Verfasser&#39;, &#39;dem&#39;, &#39;Teufel&#39;, &#39;ergeben&#39;, &#39;,&#39;, &#39;den&#39;, &#39;er&#39;, &#39;pri&#39;, &#39;##es&#39;, &#39;;&#39;, &#39;und&#39;, &#39;jetzt&#39;, &#39;erschien&#39;, &#39;der&#39;, &#39;Sad&#39;, &#39;##ismus&#39;, &#39;,&#39;, &#39;dieser&#39;, &#39;Bast&#39;, &#39;##ard&#39;, &#39;des&#39;, &#39;Kathol&#39;, &#39;##izismus&#39;, &#39;,&#39;, &#39;den&#39;, &#39;die&#39;, &#39;Religion&#39;, &#39;in&#39;, &#39;##allen&#39;, &#39;Formen&#39;, &#39;mit&#39;, &#39;Ex&#39;, &#39;##or&#39;, &#39;##zi&#39;, &#39;##sm&#39;, &#39;##en&#39;, &#39;und&#39;, &#39;Schei&#39;, &#39;##ter&#39;, &#39;##haufen&#39;, &#39;##durch&#39;, &#39;alle&#39;, &#39;Jahrhunderte&#39;, &#39;verfolgt&#39;, &#39;hat&#39;, &#39;.&#39;, &#39;Mit&#39;, &#39;Barb&#39;, &#39;##ey&#39;, &#39;d&#39;, &#39;[UNK]&#39;, &#39;[UNK]&#39;, &#39;nahm&#39;, &#39;die&#39;, &#39;Serie&#39;, &#39;der&#39;, &#39;rel&#39;, &#39;##i&#39;, &#39;[UNK]&#39;, &#39;g&#39;, &#39;##i&#39;, &#39;##ösen&#39;, &#39;Schriftsteller&#39;, &#39;ein&#39;, &#39;Ende&#39;, &#39;.&#39;, &#39;Eigentlich&#39;, &#39;gehörte&#39;, &#39;dieser&#39;, &#39;Par&#39;, &#39;##ia&#39;, &#39;in&#39;, &#39;jeder&#39;, &#39;Hinsicht&#39;, &#39;mehr&#39;, &#39;zur&#39;, &#39;welt&#39;, &#39;##lichen&#39;, &#39;Literatur&#39;, &#39;als&#39;, &#39;zu&#39;, &#39;jener&#39;, &#39;andern&#39;, &#39;,&#39;, &#39;bei&#39;, &#39;der&#39;, &#39;er&#39;, &#39;einen&#39;, &#39;Platz&#39;, &#39;##be&#39;, &#39;##anspruch&#39;, &#39;##te&#39;, &#39;,&#39;, &#39;den&#39;, &#39;man&#39;, &#39;ihm&#39;, &#39;verweigerte&#39;, &#39;.&#39;, &#39;Seine&#39;, &#39;Sprache&#39;, &#39;war&#39;, &#39;die&#39;, &#39;des&#39;, &#39;wild&#39;, &#39;##en&#39;, &#39;Roman&#39;, &#39;##ti&#39;, &#39;##sm&#39;, &#39;##us&#39;, &#39;,&#39;, &#39;voll&#39;, &#39;gew&#39;, &#39;##unde&#39;, &#39;[UNK]&#39;, &#39;ne&#39;, &#39;##r&#39;, &#39;Wend&#39;, &#39;##ungen&#39;, &#39;und&#39;, &#39;übert&#39;, &#39;##riebene&#39;, &#39;##r&#39;, &#39;Vergleich&#39;, &#39;##e&#39;, &#39;,&#39;, &#39;und&#39;, &#39;##eigent&#39;, &#39;##lich&#39;, &#39;erschien&#39;, &#39;d&#39;, &#39;[UNK]&#39;, &#39;[UNK]&#39;, &#39;wie&#39;, &#39;ein&#39;, &#39;Zucht&#39;, &#39;##hen&#39;, &#39;##gst&#39;, &#39;##unter&#39;, &#39;diesen&#39;, &#39;Wall&#39;, &#39;##achen&#39;, &#39;,&#39;, &#39;die&#39;, &#39;die&#39;, &#39;u&#39;, &#39;##lt&#39;, &#39;##ram&#39;, &#39;##ont&#39;, &#39;##anen&#39;, &#39;Stall&#39;, &#39;##e&#39;, &#39;##Dem&#39;, &#39;Herzog&#39;, &#39;kamen&#39;, &#39;diese&#39;, &#39;Betrachtung&#39;, &#39;##en&#39;, &#39;heim&#39;, &#39;##gelegen&#39;, &#39;##tlichen&#39;, &#39;Wieder&#39;, &#39;##lesen&#39;, &#39;einiger&#39;, &#39;Stellen&#39;, &#39;dieses&#39;, &#39;##C&#39;, &#39;L&#39;, &#39;(&#39;, &#39;i&#39;, &#39;##i&#39;, &#39;i&#39;, &#39;[UNK]&#39;, &#39;.&#39;, &#39;.&#39;, &#39;[UNK]&#39;, &#39;]&#39;, &#39;:&#39;, &#39;[UNK]&#39;, &#39;[UNK]&#39;, &#39;[UNK]&#39;, &#39;[UNK]&#39;, &#39;[UNK]&#39;, &#39;a&#39;, &#39;.&#39;, &#39;[UNK]&#39;, &#39;n&#39;, &#39;##ä&#39;, &#39;##ep&#39;, &#39;##n&#39;, &#39;al&#39;, &#39;##w&#39;, &#39;##xx&#39;, &#39;##r&#39;, &#39;##9&#39;, &#39;##r&#39;, &#39;##l&#39;, &#39;[UNK]&#39;, &#39;[UNK]&#39;, &#39;[UNK]&#39;] Token IDs: [185, 39, 21387, 766, 1060, 441, 9387, 26914, 173, 128, 2, 5025, 7, 7335, 2, 192, 93, 10929, 17339, 26899, 88, 18304, 8145, 9, 2, 2, 20397, 26968, 50, 2, 4189, 15845, 228, 13078, 11226, 2, 466, 144, 21, 18241, 128, 18649, 4254, 26918, 86, 67, 22074, 16, 26968, 42, 1868, 3368, 21, 16073, 1500, 26918, 534, 16804, 587, 91, 9032, 20438, 26918, 86, 30, 9373, 50, 2700, 7685, 114, 1108, 34, 517, 6694, 7, 42, 11168, 60, 26128, 4912, 987, 16902, 7547, 193, 26914, 304, 18304, 8145, 9, 2, 2, 1995, 30, 4345, 21, 4628, 26899, 2, 111, 26899, 3670, 6425, 39, 926, 26914, 13935, 2374, 534, 1059, 544, 50, 2617, 8110, 380, 252, 3522, 248, 3595, 153, 81, 8310, 19919, 26918, 178, 21, 67, 303, 1361, 165, 4465, 26, 26918, 86, 478, 787, 26792, 26914, 2072, 4247, 185, 30, 91, 24703, 7, 3529, 15099, 6694, 51, 26918, 1352, 397, 1270, 2, 2055, 26900, 16380, 184, 42, 8685, 25630, 26900, 3115, 26897, 26918, 42, 7656, 68, 3368, 9, 2, 2, 246, 39, 17373, 215, 22336, 940, 1377, 5405, 794, 26918, 30, 30, 2118, 362, 1021, 710, 6678, 16993, 26897, 12939, 5996, 3484, 620, 12115, 7, 6488, 10547, 5323, 2261, 18921, 7844, 4812, 1328, 26958, 94, 26954, 46, 26899, 46, 2, 26914, 26914, 2, 26985, 26964, 2, 2, 2, 2, 2, 18, 26914, 2, 53, 26923, 3154, 26898, 1119, 26915, 21591, 26900, 26942, 26900, 26907, 2, 2, 2] . So our bert-base-german-cased tokenizer splits the words into reasonable parts, which correspond to the token ids (input ids). . Tokenize Dataset . Let&#39;s check the length for each sequence and print the max sequence length. . max_len = 0 # For every sentence... for sent in sentences: # Tokenize the text and add `[CLS]` and `[SEP]` tokens. input_ids = tokenizer.encode(sent, add_special_tokens=True) # Update the maximum sentence length. max_len = max(max_len, len(input_ids)) print(&#39;Max sentence length: &#39;, max_len) . Token indices sequence length is longer than the specified maximum sequence length for this model (538 &gt; 512). Running this sequence through the model will result in indexing errors . Max sentence length: 538 . Das Maximum sentence length is 538. However, the maximum sentence length allowed by Bert is 512, so we have to set max_len to 256. Next, we tokenize all of our sentences. . # Tokenize all of the sentences and map the tokens to thier word IDs. input_ids = [] attention_masks = [] # For every sentence... for sent in sentences: # `encode_plus` will: # (1) Tokenize the sentence. # (2) Prepend the `[CLS]` token to the start. # (3) Append the `[SEP]` token to the end. # (4) Map tokens to their IDs. # (5) Pad or truncate the sentence to `max_length` # (6) Create attention masks for [PAD] tokens. encoded_dict = tokenizer.encode_plus( sent, # Sentence to encode. add_special_tokens = True, # Add &#39;[CLS]&#39; and &#39;[SEP]&#39; truncation=True, max_length = 256, # Pad &amp; truncate all sentences. pad_to_max_length = True, #padding=&#39;max_length=256&#39;, return_attention_mask = True, # Construct attn. masks. return_tensors = &#39;pt&#39;, # Return pytorch tensors. ) # Add the encoded sentence to the list. input_ids.append(encoded_dict[&#39;input_ids&#39;]) # And its attention mask (simply differentiates padding from non-padding). attention_masks.append(encoded_dict[&#39;attention_mask&#39;]) # Convert the lists into tensors. input_ids = torch.cat(input_ids, dim=0) attention_masks = torch.cat(attention_masks, dim=0) labels = torch.tensor(labels) . /opt/conda/envs/fastai/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1764: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding=&#39;longest&#39;` to pad to the longest sequence in the batch, or use `padding=&#39;max_length&#39;` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert). warnings.warn( . Training and Validation Set . We then put our data into a pytorch dataset and split the data into training and validation set. . from torch.utils.data import TensorDataset, random_split # Combine the training inputs into a TensorDataset. dataset = TensorDataset(input_ids, attention_masks, labels) # Create a 80-20 train-validation split. # Calculate the number of samples to include in each set. train_size = int(0.8 * len(dataset)) val_size = len(dataset) - train_size # Divide the dataset by randomly selecting samples. train_dataset, val_dataset = random_split(dataset, [train_size, val_size]) print(&#39;{:&gt;5,} training samples&#39;.format(train_size)) print(&#39;{:&gt;5,} validation samples&#39;.format(val_size)) . 75 training samples 19 validation samples . from torch.utils.data import DataLoader, RandomSampler, SequentialSampler # We take a batch size of 16 batch_size = 16 # We&#39;ll take training samples in random order. train_dataloader = DataLoader( train_dataset, sampler = RandomSampler(train_dataset), batch_size = batch_size ) # For validation the order doesn&#39;t matter, so we&#39;ll just read them sequentially. validation_dataloader = DataLoader( val_dataset, sampler = SequentialSampler(val_dataset), batch_size = batch_size ) . After putting the dataset into a dataloader we define how many different classes we&#39;ve got. . n_classes=5 . Get pretrained model . model = BertForSequenceClassification.from_pretrained( &quot;bert-base-german-cased&quot;, # Use the 12-layer BERT model, with an uncased vocab. num_labels = n_classes, # The number of output labels--2 for binary classification. # You can increase this for multi-class tasks. output_attentions = False, # Whether the model returns attentions weights. output_hidden_states = False, # Whether the model returns all hidden-states. ) # Tell pytorch to run this model on the GPU. model.cuda(); . Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForSequenceClassification: [&#39;cls.predictions.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.predictions.decoder.weight&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.seq_relationship.bias&#39;] - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model). - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: [&#39;classifier.weight&#39;, &#39;classifier.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . Optimizer and Learning Rate . We use AdamW as our optimizer and use CrossEntropyLoss as our loss function. . from transformers import AdamW . # Note: AdamW is a class from the huggingface library (as opposed to pytorch) # I believe the &#39;W&#39; stands for &#39;Weight Decay fix&quot; optimizer = AdamW(model.parameters(), lr = 2e-4, # args.learning_rate - default is 5e-5, our notebook had 2e-5 eps = 1e-8 # args.adam_epsilon - default is 1e-8. ) . from transformers import get_linear_schedule_with_warmup epochs = 5 # Total number of training steps is [number of batches] x [number of epochs]. # (Note that this is not the same as the number of training samples). total_steps = len(train_dataloader) * epochs # Create the learning rate scheduler. scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 10, # Default value in run_glue.py num_training_steps = total_steps) . loss_fn = nn.CrossEntropyLoss().to(device) . def format_time(elapsed): &#39;&#39;&#39; Takes a time in seconds and returns a string hh:mm:ss &#39;&#39;&#39; # Round to the nearest second. elapsed_rounded = int(round((elapsed))) # Format as hh:mm:ss return str(datetime.timedelta(seconds=elapsed_rounded)) . Train Model . Let&#39;s train our model! . import random import numpy as np import time import datetime seed_val = 42 random.seed(seed_val) np.random.seed(seed_val) torch.manual_seed(seed_val) torch.cuda.manual_seed_all(seed_val) # We&#39;ll store a number of quantities such as training and validation loss, # validation accuracy, and timings. training_stats = [] # Measure the total training time for the whole run. total_t0 = time.time() # For each epoch... for epoch_i in range(0, epochs): # ======================================== # Training # ======================================== # Perform one full pass over the training set. print(&quot;&quot;) print(&#39;======== Epoch {:} / {:} ========&#39;.format(epoch_i + 1, epochs)) print(&#39;Training...&#39;) # Measure how long the training epoch takes. t0 = time.time() # Reset the total loss for this epoch. total_train_loss = 0 model.train() # For each batch of training data... for step, batch in enumerate(train_dataloader): # Progress update every 40 batches. if step % 40 == 0 and not step == 0: # Calculate elapsed time in minutes. elapsed = format_time(time.time() - t0) # Report progress. print(f&#39;Batch:{step} of {len(train_dataloader)}. Elapsed: {elapsed}&#39;) # `batch` contains three pytorch tensors: # [0]: input ids # [1]: attention masks # [2]: labels b_input_ids = batch[0].to(device) b_input_mask = batch[1].to(device) b_labels = batch[2].to(device) model.zero_grad() loss, logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels) total_train_loss += loss.item() loss.backward() # Clip the norm of the gradients to 1.0. # This is to help prevent the &quot;exploding gradients&quot; problem. torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) optimizer.step() # Update the learning rate. scheduler.step() # Calculate the average loss over all of the batches. avg_train_loss = total_train_loss / len(train_dataloader) # Measure how long this epoch took. training_time = format_time(time.time() - t0) print(&quot;&quot;) print(&quot; Average training loss: {0:.2f}&quot;.format(avg_train_loss)) print(&quot; Training epoch took: {:}&quot;.format(training_time)) # ======================================== # Validation # ======================================== # After the completion of each training epoch, measure our performance on # our validation set. print(&quot;&quot;) print(&quot;Running Validation...&quot;) t0 = time.time() # Put the model in evaluation mode--the dropout layers behave differently # during evaluation. model.eval() # Tracking variables total_eval_accuracy = 0 total_eval_loss = 0 nb_eval_steps = 0 # Evaluate data for one epoch for batch in validation_dataloader: b_input_ids = batch[0].to(device) b_input_mask = batch[1].to(device) b_labels = batch[2].to(device) (loss, logits) = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels) # Accumulate the validation loss. total_eval_loss += loss.item() # Calculate the average loss over all of the batches. avg_val_loss = total_eval_loss / len(validation_dataloader) # Measure how long the validation run took. validation_time = format_time(time.time() - t0) print(f&#39;Validation Loss: {avg_val_loss}&#39;) print(f&#39;Validation took: {validation_time}&#39;) # Record all statistics from this epoch. training_stats.append( { &#39;epoch&#39;: epoch_i + 1, &#39;Training Loss&#39;: avg_train_loss, &#39;Valid. Loss&#39;: avg_val_loss, &#39;Training Time&#39;: training_time, &#39;Validation Time&#39;: validation_time } ) print(&quot;&quot;) print(&quot;Training complete!&quot;) print(&quot;Total training took {:} (h:mm:ss)&quot;.format(format_time(time.time()-total_t0))) . ======== Epoch 1 / 5 ======== Training... Average training loss: 1.52 Training epoch took: 0:00:07 Running Validation... Validation Loss: 1.3076387345790863 Validation took: 0:00:01 ======== Epoch 2 / 5 ======== Training... Average training loss: 1.34 Training epoch took: 0:00:07 Running Validation... Validation Loss: 1.1822895407676697 Validation took: 0:00:01 ======== Epoch 3 / 5 ======== Training... Average training loss: 0.85 Training epoch took: 0:00:07 Running Validation... Validation Loss: 0.8835422396659851 Validation took: 0:00:01 ======== Epoch 4 / 5 ======== Training... Average training loss: 0.32 Training epoch took: 0:00:07 Running Validation... Validation Loss: 0.46347731351852417 Validation took: 0:00:01 ======== Epoch 5 / 5 ======== Training... Average training loss: 0.13 Training epoch took: 0:00:07 Running Validation... Validation Loss: 0.4694706201553345 Validation took: 0:00:01 Training complete! Total training took 0:00:40 (h:mm:ss) . Evaluation . # Display floats with two decimal places. pd.set_option(&#39;precision&#39;, 2) # Create a DataFrame from our training statistics. df_stats = pd.DataFrame(data=training_stats) # Use the &#39;epoch&#39; as the row index. df_stats = df_stats.set_index(&#39;epoch&#39;) # Display the table. df_stats . Training Loss Valid. Loss Training Time Validation Time . epoch . 1 1.52 | 1.31 | 0:00:07 | 0:00:01 | . 2 1.34 | 1.18 | 0:00:07 | 0:00:01 | . 3 0.85 | 0.88 | 0:00:07 | 0:00:01 | . 4 0.32 | 0.46 | 0:00:07 | 0:00:01 | . 5 0.13 | 0.47 | 0:00:07 | 0:00:01 | . import matplotlib.pyplot as plt import seaborn as sns # Use plot styling from seaborn. sns.set(style=&#39;darkgrid&#39;) # Increase the plot size and font size. sns.set(font_scale=1.5) plt.rcParams[&quot;figure.figsize&quot;] = (12,6) # Plot the learning curve. plt.plot(df_stats[&#39;Training Loss&#39;], &#39;b-o&#39;, label=&quot;Training&quot;) plt.plot(df_stats[&#39;Valid. Loss&#39;], &#39;g-o&#39;, label=&quot;Validation&quot;) # Label the plot. plt.title(&quot;Training &amp; Validation Loss&quot;) plt.xlabel(&quot;Epoch&quot;) plt.ylabel(&quot;Loss&quot;) plt.legend() plt.xticks([1, 2, 3, 4]) plt.show() . Look at results . So far, we can see that our model learns. Let&#39;s have a look how the predictions on our trainloader look like: . # Prediction on test set # Put model in evaluation mode model.eval() # Tracking variables predictions , true_labels = [], [] # Predict for batch in train_dataloader: # Add batch to GPU batch = tuple(t.to(device) for t in batch) # Unpack the inputs from our dataloader b_input_ids, b_input_mask, b_labels = batch with torch.no_grad(): # Forward pass, calculate logit predictions outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask) logits = outputs[0] # Move logits and labels to CPU logits = logits.detach().cpu().numpy() label_ids = b_labels.to(&#39;cpu&#39;).numpy() # Store predictions and true labels predictions.append(logits) true_labels.append(label_ids) print(&#39; DONE.&#39;) . DONE. DONE. DONE. DONE. DONE. . np.argmax(predictions[4], axis=1) . array([0, 4, 4, 0, 4, 3, 3, 3, 4, 3, 4]) . true_labels[4] . array([0, 4, 4, 0, 4, 3, 3, 3, 4, 3, 4]) . Well that looks right. However, this is the data we trained our model on. Way more interesting is the data our model hasn&#39;t trained on: . # Prediction on test set # Put model in evaluation mode model.eval() # Tracking variables predictions , true_labels = [], [] # Predict for batch in validation_dataloader: # Add batch to GPU batch = tuple(t.to(device) for t in batch) # Unpack the inputs from our dataloader b_input_ids, b_input_mask, b_labels = batch with torch.no_grad(): # Forward pass, calculate logit predictions outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask) logits = outputs[0] # Move logits and labels to CPU logits = logits.detach().cpu().numpy() label_ids = b_labels.to(&#39;cpu&#39;).numpy() # Store predictions and true labels predictions.append(logits) true_labels.append(label_ids) print(&#39; DONE.&#39;) . DONE. DONE. . np.argmax(predictions[0], axis=1) . array([4, 3, 2, 1, 4, 0, 4, 1, 0, 1, 3, 4, 3, 4, 1, 4]) . true_labels[0] . array([3, 3, 1, 1, 4, 0, 0, 1, 0, 1, 3, 4, 3, 4, 1, 4]) . Awesome! We have quite some variation in our prediction and most of the time they look pretty good! We save our model: . Save Model . torch.save(model.state_dict(), p/&#39;model/model_5epochs_lr1e-4.pt&#39;) . To further improve the model, I will get more data and then retrain it. However, I think so far the model does a pretty good job in replacing me when it comes to book recommendations. . In the next part I will show you how to use Binder to make a small web application in which we can upload a picture of a page and then make a prediction how many stars I would probably give this book. . So stay tuned for the next blogpost! . Lasse .",
            "url": "https://lschmiddey.github.io/fastpages_/2020/09/26/Use-pretrained-Bert-model-for-classification-Part2.html",
            "relUrl": "/2020/09/26/Use-pretrained-Bert-model-for-classification-Part2.html",
            "date": " • Sep 26, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Teach Python how to read",
            "content": "Using PIL and pytesseract . This is the first blogpost of a three to four (I haven&#39;t decided yet) part project. The main idea is that I want to create a model which will tell me how much I would like the book, given an image of a page as in input. . In this part, I will show you how to turn a image of text into actual text, using pytesseract. So let&#39;s first get our packages. . Import packages . !apt-get update !apt-get install libleptonica-dev -y !apt-get install tesseract-ocr tesseract-ocr-dev -y !apt-get install libtesseract-dev -y !apt-get install tesseract-ocr-deu . Ign:1 http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64 InRelease Hit:2 http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64 Release Hit:3 http://security.ubuntu.com/ubuntu xenial-security InRelease Ign:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64 InRelease Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64 Release Hit:7 http://archive.ubuntu.com/ubuntu xenial InRelease Hit:9 http://archive.ubuntu.com/ubuntu xenial-updates InRelease Hit:10 http://archive.ubuntu.com/ubuntu xenial-backports InRelease Reading package lists... Done Reading package lists... Done Building dependency tree Reading state information... Done libleptonica-dev is already the newest version (1.73-1). 0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded. Reading package lists... Done Building dependency tree Reading state information... Done tesseract-ocr is already the newest version (3.04.01-4). tesseract-ocr-dev is already the newest version (3.04.01-4). 0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded. Reading package lists... Done Building dependency tree Reading state information... Done libtesseract-dev is already the newest version (3.04.01-4). 0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded. Reading package lists... Done Building dependency tree Reading state information... Done The following NEW packages will be installed: tesseract-ocr-deu 0 upgraded, 1 newly installed, 0 to remove and 39 not upgraded. Need to get 4153 kB of archives. After this operation, 13.4 MB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu xenial/universe amd64 tesseract-ocr-deu all 3.04.00-1 [4153 kB] Fetched 4153 kB in 1s (2333 kB/s) debconf: delaying package configuration, since apt-utils is not installed Selecting previously unselected package tesseract-ocr-deu. (Reading database ... 18655 files and directories currently installed.) Preparing to unpack .../tesseract-ocr-deu_3.04.00-1_all.deb ... Unpacking tesseract-ocr-deu (3.04.00-1) ... Setting up tesseract-ocr-deu (3.04.00-1) ... . !pip install pytesseract . Requirement already satisfied: pytesseract in /opt/conda/envs/fastai/lib/python3.8/site-packages (0.3.6) Requirement already satisfied: Pillow in /opt/conda/envs/fastai/lib/python3.8/site-packages (from pytesseract) (7.2.0) . from fastai.vision.all import * from fastai.vision.widgets import * import numpy as np from pathlib import Path import pytesseract import re from PIL import Image, ImageFilter import pandas as pd . Define PosixPath to data . I took about 100 images of pages from books I own (all in German). I then put them in an image folder, let&#39;s have a look at the directory. . p = Path.cwd()/&#39;images&#39; . img_paths = [x for x in p.iterdir()] . img_paths[0].parts[5] . &#39;IMG_5123_gegendenStrich.jpg&#39; . Use regex to extract title of book . Next to the text from the image, I would like to extract the title of the book so I can later easily join my ratings to the texts. I therefore use a regex. . title_list = [re.match(&quot;^.* _(.*) ..*$&quot;,img_paths[i].parts[5]).group(1) for i in range(len(img_paths))] . Automatically read in images, transform them and get text . We use pytesseract for extracting the text from the images. To improve the performance I tried a lot of data transformation: cropping, binarizing and a lot more. The only thing that worked for me was to first rotate the image and then use a MedianFilter. . def proc_img(img_path): im1 = Image.open(img_path) im1 = im1.rotate(angle=270, resample=0, expand=10, center=None, translate=None, fillcolor=None) im1 = im1.filter(ImageFilter.MedianFilter) return im1 . All of my input images are from german books, so I need to use lang=&quot;deu&quot;. . def get_text(img): return pytesseract.image_to_string(img, lang=&quot;deu&quot;) . I again use a regular expression to get rid of common mistakes pytesseract does: putting a n somewhere or confusing a s for a 5. . def use_pattern(text): return pattern.sub(lambda m: rep[re.escape(m.group(0))], text) . rep = {&quot; n&quot;: &quot;&quot;, &quot;`&quot;: &quot;&quot;, &#39;%&#39;:&quot;&quot;, &#39;°&#39;: &#39;&#39;, &#39;&amp;&#39;:&#39;&#39;, &#39;‘&#39;:&#39;&#39;, &#39;€&#39;:&#39;e&#39;, &#39;®&#39;:&#39;&#39;, &#39; &#39;: &#39;&#39;, &#39;5&#39;:&#39;s&#39;, &#39;1&#39;:&#39;i&#39;, &#39;_&#39;:&#39;&#39;, &#39;-&#39;:&#39;&#39;} # define desired replacements here # use these three lines to do the replacement rep = dict((re.escape(k), v) for k, v in rep.items()) #Python 3 renamed dict.iteritems to dict.items so use rep.items() for latest versions pattern = re.compile(&quot;|&quot;.join(rep.keys())) . Use tesseract to make image into text . Finally, we use a list comprehension (they&#39;re super useful) to put all of the text into a list of texts. . text_list = [use_pattern(get_text(proc_img(str(img_paths[i])))) for i in range(len(img_paths))] . Combine into Dataframe . And now let&#39;s put that into a pandas dataframe. . d = {&#39;text&#39;:text_list,&#39;title&#39;:title_list} df = pd.DataFrame(d) df.head() . text title . 0 war ein schrecklicher Rückfall eingetreten.In dem »verheirateten Priester« wurde das LobChristi von Barbey d’Aurévilly gesungen; in »LesDiaboliques« hatte sich der Verfasser dem Teufel ergeben, den er pries; und jetzt erschien der Sadismus,dieser Bastard des Katholizismus, den die Religion inallen Formen mit Exorzismen und Scheiterhaufendurch alle Jahrhunderte verfolgt hat.Mit Barbey d’Aurévilly nahm die Serie der reli—giösen Schriftsteller ein Ende. Eigentlich gehörte dieser Paria in jeder Hinsicht mehr zur weltlichen Lite—ratur als zu jener andern, bei der er einen Platzbeanspruchte, den... | gegendenStrich | . 1 höchst moralischer Akt, die Welt von einem solchen Ungeheuerzu befreien? Die Menschheit wäre dann besser und glücklicherund das Leben schöner und süßer.Ich dachte lange darüber nach. Schlaﬂos lag ich in meinerKoj e und hielt mir die Fakten vor Augen. Mit Johnson und Leachredete ich während der Nachtwachen, wenn Wolf Larsen unterDeck war. Beide Männer hatten die Hoffnung verloren — Johnson wegen seiner grundlegenden Schwermut, Leach, weil ersich in seinen vergeblichen Kämpfen erschöpft hatte. Aber eines Nachts ergriff er leidenschaftlich meine Hand und sagte:»Ich glaube, Sie sind in Ordnung... | derSeewolf | . 2 deutsches Luder nehmen. Und sollten Sie es dann allzu eilighaben, Mr. Andrews, werden Sie mich wohl von ihr runterziehen müssen.«Andrews wartete, dass die beiden Männer fortritten, undsah ihnen nach, wie sie sich durch das dämmrige Tal entfern—ten und ihre auf und ab wippenden Gestalten mit der dunkleren Schraffur der westlichen Berghänge verschmelzen. Dannaufs Neue überraschte, lenkte die Hände ab und sorgte dafür,dass ihm die eigenen Gesichtszüge fremd vorkamen; er fragtesich, wie er wohl aussah, fragte sich, ob Francine ihn wiederkennen würde, wenn sie ihn jetzt sehen könnte.Seit dem Ab... | ButchersCrossing | . 3 Doktor, das wissen Sie so gut wie ich. Vor hundert Jah—ren hat eine Pestepidemie in Persien alle Bewohner einerStadt getötet und ausgerechnet den Totenwäscher nicht,der nie aufgehört hatte, seine Arbeit zu verrichten.»müssen.»Sie kamen jetzt in die Vorstadt. Die Scheinwerfer beleuchteten die menschenleeren Straßen. Sie hielten an.Vor dem Auto fragte Rieux Tarrou, ob er mitkommenwolle, und der sagte ja. Ein Schimmer vom Himmel er—hellte ihre Gesichter. Rieux lachte plötzlich freundschaftlich.« Sagen Sie, Tarrou, was treibt Sie dazu, sich damit zubefassen? »&lt;&lt; Ich weiß nicht. Meine Moral vie... | diePest | . 4 ins Gesicht, wandte sich von ihrem traurigen Ausdruckangewidert ab, und nachdem er zum hundertsten Maldie Aushängeschilder der gegenüberliegenden Geschäfte, die Werbung für die schon nicht mehr erhält—lichen bekannten Aperitifmarken gelesen hatte, stander auf und lief ziellos durch die gelben Straßen derStadt. Er schleppte sich von einsamen Spaziergängen inCafés und von Cafés in Restaurants und erreichte soden Abend. An einem solchen Abend sah Rieux denJournalisten zögernd vor der Tür eines Cafés stehen. Erschien sich zu entschließen, ging hinein und setzte sichhinten hin. Es war um die Ze... | diePest | . df.text[3] . &#39;Doktor, das wissen Sie so gut wie ich. Vor hundert Jah—ren hat eine Pestepidemie in Persien alle Bewohner einerStadt getötet und ausgerechnet den Totenwäscher nicht,der nie aufgehört hatte, seine Arbeit zu verrichten.»müssen.»Sie kamen jetzt in die Vorstadt. Die Scheinwerfer beleuchteten die menschenleeren Straßen. Sie hielten an.Vor dem Auto fragte Rieux Tarrou, ob er mitkommenwolle, und der sagte ja. Ein Schimmer vom Himmel er—hellte ihre Gesichter. Rieux lachte plötzlich freundschaftlich.« Sagen Sie, Tarrou, was treibt Sie dazu, sich damit zubefassen? »&lt;&lt; Ich weiß nicht. Meine Moral vielleicht.»«Und die wäre? &gt;&gt;«Verständnis.»Tarrou wandte sich dem Haus zu, und Rieux sah erstwieder sein Gesicht, als sie bei dem alten Asthmatikerwaren.&#39; . Looking good! For this project I also need my ratings for each of the books. I use a dictionary and the map function to easily create a column with my ratings. . Use Dictionary to map my ratings . rating_lasse = {&#39;derPate&#39;: 5, &#39;ButchersCrossing&#39;: 4, &#39;derSeewolf&#39;: 5, &#39;JekyllandHyde&#39;: 4, &#39;gegendenStrich&#39;: 1, &#39;FruestueckmitKaengurus&#39;: 5, &#39;HuckleberryFinn&#39;: 4, &#39;diePest&#39;: 2, &#39;HerzderFinsternis&#39;: 3, &#39;derSpieler&#39;: 4} . df[&#39;rating&#39;] = df[&#39;title&#39;].map(rating_lasse) . df.head() . text title rating . 0 war ein schrecklicher Rückfall eingetreten.In dem »verheirateten Priester« wurde das LobChristi von Barbey d’Aurévilly gesungen; in »LesDiaboliques« hatte sich der Verfasser dem Teufel ergeben, den er pries; und jetzt erschien der Sadismus,dieser Bastard des Katholizismus, den die Religion inallen Formen mit Exorzismen und Scheiterhaufendurch alle Jahrhunderte verfolgt hat.Mit Barbey d’Aurévilly nahm die Serie der reli—giösen Schriftsteller ein Ende. Eigentlich gehörte dieser Paria in jeder Hinsicht mehr zur weltlichen Lite—ratur als zu jener andern, bei der er einen Platzbeanspruchte, den... | gegendenStrich | 1 | . 1 höchst moralischer Akt, die Welt von einem solchen Ungeheuerzu befreien? Die Menschheit wäre dann besser und glücklicherund das Leben schöner und süßer.Ich dachte lange darüber nach. Schlaﬂos lag ich in meinerKoj e und hielt mir die Fakten vor Augen. Mit Johnson und Leachredete ich während der Nachtwachen, wenn Wolf Larsen unterDeck war. Beide Männer hatten die Hoffnung verloren — Johnson wegen seiner grundlegenden Schwermut, Leach, weil ersich in seinen vergeblichen Kämpfen erschöpft hatte. Aber eines Nachts ergriff er leidenschaftlich meine Hand und sagte:»Ich glaube, Sie sind in Ordnung... | derSeewolf | 5 | . 2 deutsches Luder nehmen. Und sollten Sie es dann allzu eilighaben, Mr. Andrews, werden Sie mich wohl von ihr runterziehen müssen.«Andrews wartete, dass die beiden Männer fortritten, undsah ihnen nach, wie sie sich durch das dämmrige Tal entfern—ten und ihre auf und ab wippenden Gestalten mit der dunkleren Schraffur der westlichen Berghänge verschmelzen. Dannaufs Neue überraschte, lenkte die Hände ab und sorgte dafür,dass ihm die eigenen Gesichtszüge fremd vorkamen; er fragtesich, wie er wohl aussah, fragte sich, ob Francine ihn wiederkennen würde, wenn sie ihn jetzt sehen könnte.Seit dem Ab... | ButchersCrossing | 4 | . 3 Doktor, das wissen Sie so gut wie ich. Vor hundert Jah—ren hat eine Pestepidemie in Persien alle Bewohner einerStadt getötet und ausgerechnet den Totenwäscher nicht,der nie aufgehört hatte, seine Arbeit zu verrichten.»müssen.»Sie kamen jetzt in die Vorstadt. Die Scheinwerfer beleuchteten die menschenleeren Straßen. Sie hielten an.Vor dem Auto fragte Rieux Tarrou, ob er mitkommenwolle, und der sagte ja. Ein Schimmer vom Himmel er—hellte ihre Gesichter. Rieux lachte plötzlich freundschaftlich.« Sagen Sie, Tarrou, was treibt Sie dazu, sich damit zubefassen? »&lt;&lt; Ich weiß nicht. Meine Moral vie... | diePest | 2 | . 4 ins Gesicht, wandte sich von ihrem traurigen Ausdruckangewidert ab, und nachdem er zum hundertsten Maldie Aushängeschilder der gegenüberliegenden Geschäfte, die Werbung für die schon nicht mehr erhält—lichen bekannten Aperitifmarken gelesen hatte, stander auf und lief ziellos durch die gelben Straßen derStadt. Er schleppte sich von einsamen Spaziergängen inCafés und von Cafés in Restaurants und erreichte soden Abend. An einem solchen Abend sah Rieux denJournalisten zögernd vor der Tür eines Cafés stehen. Erschien sich zu entschließen, ging hinein und setzte sichhinten hin. Es war um die Ze... | diePest | 2 | . And that&#39;s it, let&#39;s save this dataframe and we&#39;re ready to move on to the model training! . df.to_csv(p/&#39;datasets/text_df.csv&#39;, encoding=&#39;utf8&#39;, index=False) . I hope you enjoyed this blogpost and stay tuned for the next one! . Lasse .",
            "url": "https://lschmiddey.github.io/fastpages_/2020/09/22/Build_Dataset_from_Images-Part1.html",
            "relUrl": "/2020/09/22/Build_Dataset_from_Images-Part1.html",
            "date": " • Sep 22, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "How to classify Lego figures",
            "content": "What better way to impress your significant other? . Build a Lego Classifier with fastai . !pip install kaggle . We want to save the dataset into the folder /notebooks/storage/data/Lego-Classification: . !kaggle datasets download -d ihelon/lego-minifigures-classification -p /notebooks/storage/data/Lego-Classification . Unzip data using Pythons pathlib library . from pathlib import Path p = Path(&#39;/notebooks/storage/data/Lego-Classification&#39;) filename = Path(&#39;/notebooks/storage/data/Lego-Classification/lego-minifigures-classification.zip&#39;) . Just as before, we can use bash commands from within Jupyter Notebook. So let&#39;s do that to unzip our data. -q is quiet mode, -d points to the direction where to unzip the data. Just see how well Pythons pathlib and bash work together! . !unzip -q {str(filename)} -d {str(p/&quot;train&quot;)} . Imports . from fastbook import * from fastai.vision.widgets import * import pandas as pd . Let&#39;s now use fastai&#39;s &quot;get_image_files()&quot; function to see how the unzipped data looks like in our destination path: . fns = get_image_files(p/&quot;train&quot;) fns . (#316) [Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/009.jpg&#39;),Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/010.jpg&#39;),Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/006.jpg&#39;),Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/001.jpg&#39;),Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/011.jpg&#39;),Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/005.jpg&#39;),Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/004.jpg&#39;),Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/013.jpg&#39;),Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/007.jpg&#39;),Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/008.jpg&#39;)...] . Remember, we put the data into our directory &#39;/notebooks/storage/data/Lego-Classification&#39;. After having a quick look at our data it looks like the data is stored as follows: first the genre of our image (marvel/jurassic-world), then the classification of the figure (0001/0002 etc.). Within these folders we find many different pictures of that figure (001.jpg/002.jpg and so on). Let&#39;s confirm this by looking at the metadata. . df = pd.read_csv(f&#39;{p}/index.csv&#39;, index_col=0) df.tail(5) . path class_id train-valid . 311 marvel/0014/008.jpg | 28 | valid | . 312 marvel/0014/009.jpg | 28 | valid | . 313 marvel/0014/010.jpg | 28 | valid | . 314 marvel/0014/011.jpg | 28 | valid | . 315 marvel/0014/012.jpg | 28 | valid | . df_metadata = pd.read_csv(f&#39;{p}/metadata.csv&#39;, usecols=[&#39;class_id&#39;, &#39;lego_names&#39;, &#39;minifigure_name&#39;]) df_metadata.head() . class_id lego_names minifigure_name . 0 1 | [&#39;Spider Mech vs. Venom&#39;] | SPIDER-MAN | . 1 2 | [&#39;Spider Mech vs. Venom&#39;] | VENOM | . 2 3 | [&#39;Spider Mech vs. Venom&#39;] | AUNT MAY | . 3 4 | [&#39;Spider Mech vs. Venom&#39;] | GHOST SPIDER | . 4 5 | [&quot;Yoda&#39;s Hut&quot;] | YODA | . Indeed, that&#39;s how this dataset is structured. What we want is a data structure with which fastai&#39;s data block can easily work with. So what we need is something that gives us the filename, the label and a label which data is for training and which one is for validation. Luckily we can get exactly this by combining the meta-data: . datablock_df = pd.merge(df, df_metadata, left_on=&#39;class_id&#39;, right_on=&#39;class_id&#39;).loc[:,[&#39;path&#39;, &#39;class_id&#39;, &#39;minifigure_name&#39;, &#39;train-valid&#39;]] datablock_df[&#39;is_valid&#39;] = datablock_df[&#39;train-valid&#39;]==&#39;valid&#39; datablock_df.head() . path class_id minifigure_name train-valid is_valid . 0 marvel/0001/001.jpg | 1 | SPIDER-MAN | train | False | . 1 marvel/0001/002.jpg | 1 | SPIDER-MAN | valid | True | . 2 marvel/0001/003.jpg | 1 | SPIDER-MAN | train | False | . 3 marvel/0001/004.jpg | 1 | SPIDER-MAN | train | False | . 4 marvel/0001/005.jpg | 1 | SPIDER-MAN | train | False | . fastai gives us a brief overview of what to check before we can make optimal use of the datablock: . what are the types of our inputs and targets? Images and labels. where is the data? In a dataframe. how do we know if a sample is in the training or the validation set? A column of our dataframe. how do we get an image? By looking at the column path. how do we know the label of an image? By looking at the column minifigure_name. do we want to apply a function to a given sample? Yes, we need to resize everything to a given size. do we want to apply a function to a batch after it&#39;s created? Yes, we want data augmentation. . lego_block = DataBlock(blocks=(ImageBlock, CategoryBlock), splitter=ColSplitter(), get_x=lambda x:p/&quot;train&quot;/f&#39;{x[0]}&#39;, get_y=lambda x:x[2], item_tfms=Resize(224), batch_tfms=aug_transforms()) . Now our datablock is called lego_block. See how it perfectly matches together? . Let me briefly explain what the different steps within our lego_block are doing: first we tell the lego_block on what we want to split our data on (the default here is col=&#39;is_valid&#39;), then we simply put our path column (x[0]) and combine it with our path p and the folder &#39;train&#39; in which is is located in. get_y tells the lego_block where to find the labels in our dataset (x[2]), we then make all of our images the same size and apply transformation on them (checkout fastai for more information). . dls = lego_block.dataloaders(datablock_df) dls.show_batch() . Glorious! . fastai tries to make our life easier. This blog is intended to show you guys how to easily and quickly manage to get a great classifier with it. In the upcoming blogs I will try to better explain what is going on behind the scenes. But for now, let&#39;s enjoy how fast we can build our classifier with fastai! . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(20) . epoch train_loss valid_loss error_rate time . 0 | 5.044340 | 7.069784 | 0.980263 | 00:04 | . epoch train_loss valid_loss error_rate time . 0 | 4.333549 | 5.407881 | 0.960526 | 00:05 | . 1 | 4.193551 | 4.404543 | 0.947368 | 00:04 | . 2 | 3.994802 | 3.605752 | 0.907895 | 00:04 | . 3 | 3.721432 | 2.941005 | 0.802632 | 00:04 | . 4 | 3.428093 | 2.336661 | 0.638158 | 00:05 | . 5 | 3.092875 | 1.841397 | 0.532895 | 00:04 | . 6 | 2.779624 | 1.470707 | 0.434211 | 00:05 | . 7 | 2.484196 | 1.200388 | 0.348684 | 00:05 | . 8 | 2.216630 | 1.011226 | 0.263158 | 00:05 | . 9 | 1.988927 | 0.901224 | 0.236842 | 00:04 | . 10 | 1.794780 | 0.819948 | 0.217105 | 00:05 | . 11 | 1.624516 | 0.756500 | 0.184211 | 00:04 | . 12 | 1.478024 | 0.716581 | 0.157895 | 00:05 | . 13 | 1.354157 | 0.688189 | 0.164474 | 00:04 | . 14 | 1.244102 | 0.673431 | 0.164474 | 00:05 | . 15 | 1.149104 | 0.662224 | 0.164474 | 00:04 | . 16 | 1.062147 | 0.652154 | 0.164474 | 00:04 | . 17 | 0.985224 | 0.654423 | 0.177632 | 00:04 | . 18 | 0.917764 | 0.653068 | 0.177632 | 00:05 | . 19 | 0.858115 | 0.652137 | 0.184211 | 00:04 | . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . interp.most_confused(min_val=2) . [(&#39;RON WEASLEY&#39;, &#39;HARRY POTTER&#39;, 2), (&#39;SPIDER-MAN&#39;, &#39;FIREFIGHTER&#39;, 2)] . Not to bad I would say. However, seeing an image of Ronald Weasley and predicting it to be Harry Potter - I&#39;m not sure how much this will impress your significant other. On the other hand, Captain America is correctly predicted 100%. . But we can still try to improve our model by unfreezing the weights, to make the model even better. Let&#39;s check this out: . learn.fit_one_cycle(3, 3e-3) . epoch train_loss valid_loss error_rate time . 0 | 0.061034 | 0.536347 | 0.151316 | 00:05 | . 1 | 0.046480 | 0.631286 | 0.157895 | 00:04 | . 2 | 0.039729 | 0.572698 | 0.157895 | 00:05 | . Then we will unfreeze the parameters and learn at a slightly lower learning rate: . learn.unfreeze() . learn.fit_one_cycle(2, lr_max=1e-5) . epoch train_loss valid_loss error_rate time . 0 | 0.014817 | 0.483588 | 0.125000 | 00:05 | . 1 | 0.016834 | 0.425858 | 0.098684 | 00:04 | . Wow! Down to only 10% error rate. I think that&#39;s quite impressive! Let&#39;s see the confusion matrix: . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . With these results I am sure you will impress your significant other! . In one of the next posts I will show you how to use Jupyter to easily set up a small Web App with the help of Binder. So stay tuned! . Lasse .",
            "url": "https://lschmiddey.github.io/fastpages_/2020/09/16/Lego-Classification.html",
            "relUrl": "/2020/09/16/Lego-Classification.html",
            "date": " • Sep 16, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "How to get the kaggle API started",
            "content": "Easy way to download datasets from kaggle from your terminal . In this blog post I would like to show you how to use kaggles api. And not only that, I will show you how to directly use this API from Jupyter. It&#39;s straightforward and shouldn&#39;t take longer than 5 minutes. Let&#39;s get started! . First of all, log in into your kaggle account and go to &quot;Your Account&quot;. Scroll down until you find the section API. Click on &quot;Create New API Token&quot;. . . Next, upload your downloaded token into a directory to which you have access to. In this blogpost, I use my Cloud Working Space on Paperspace (how to easily set up your own working space in Paperspace click here. . . Note however, that kaggle expects your token to be at a specific folder: ~/.kaggle/kaggle.json on Linux, OSX, and other UNIX-based operating systems, and at C: Users&lt;Windows-username.kaggle kaggle.json on Windows. So that&#39;s what we will do next. Just open your Terminal and move your token to the expected folder. . . And that&#39;s it, you can now use the kaggle API! If you (like me) use a remote computing instance, we don&#39;t want other user to possible use our token. We can prevent this by typing: . . Let&#39;s see how the kaggle API works. First of all, we need to pip install the package. . . If you are looking for a specific dataset, you can now use the kaggle API and simply type: . . If you want to download data from this API, you write: . . In that way you can easily access kaggle datasets and make that work even on cloud computing instances. . Stay tuned for the next blogpost! . Lasse .",
            "url": "https://lschmiddey.github.io/fastpages_/2020/09/15/get-kaggle-api-started.html",
            "relUrl": "/2020/09/15/get-kaggle-api-started.html",
            "date": " • Sep 15, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Setup Paperspace",
            "content": "Kostenlose (und bessere?) Alternative zu Google Colab . Die meisten Machine Learning bzw. Deep Learning Projekte will man nicht auf seinem eigenem PC/Laptop rechnen lassen. Dies hat viele gute Gründe. Zum einen hat nicht jeder Laptop eine geeignete Grafikkarte (GPU), auf der sich DeepLearning Modelle berechnen lassen. Außerdem ist das administrieren der verschiedenen Treiber und das Setup der GPU für Jupyter Notebooks nichts, was super einfach ist. Im Gegenteil. Zudem kostet eine gute Grafikkarte viel Geld. Alles Gründe, weswegen man sich lieber nach kostengünstigen Alternativen umsehen sollte, die einem den administrativen Aufwand abnehmen. . Eine solche Alternative ist Paperspace. Paperspace Gradient stellt ähnlich zu Google Colab eine kostenlose Alternative, wie sich in der Cloud Rechner mit GPUs benutzen lassen. Außerdem hat Paperspace Gradient den Vorteil, dass sich auch Daten wie Datensätze, Bilder, Modelle kostenlos speichern lassen, und nicht nach beenden der Instanz gelöscht werden (wie zum Beispiel in Google Colab). . In diesem Blog will ich zeigen, wie sich über Paperspace eine kostenlose Instanz anlegen lässt, die auch mit GPUs läuft, und wie sich die erstellten Notebooks einfach mit einem bestehenden github Repo verbinden lassen. . Zunächst muss man sich auf der Paperspace-Seite registrieren. Nach der einmaligen Registrierung öffnet sich Folgendes: . . Hier muss zunächst der Container gewählt werden. Das bedeutet nichts anderes, als dass man ein für ein bestimmtes Projekt benötigtes Grund-Setup schon für einen vorinstalliert bekommt - wie wunderbar! Man muss sich über keine Abhängigkeiten kümmern, ist sich sicher, dass die Packages auf dem aktuellsten Stand sind und falls benötigt kann man später immer noch selbst Packages hinzufügen. Was für ein Luxus! Wenn man z.B. ein Projekt mit PyTorch bauen will, so klickt man einfach auf den PyTorch Container. . . Als nächstes wählt man die Maschine, auf der das Projekt laufen soll. Das Beste hierbei: es gibt Maschinen, die von Paperspace komplett umsonst zur Verfügung gestellt werden, sogar mit GPU! Diese sind die letzten 3 Auswahlmöglichkeiten. Und wenn man mehr Power braucht, so kann man auch im Nachhinein noch aufrüsten. . Des weiteren kann man ganz einfach ein bestehendes eigenes git-Repo mit der Instanz verbinden. Dafür einfach den git-Link zum Repo einfügen. . . Zuletzt muss noch eine gültige Kreditkarte hinterlegt werden (wie schon beschrieben, die Kreditkarte wird nur belastet, wenn eine der nicht kotenlosen Alternativen gewählt wird). Danach kann auf &quot;Create new Instance&quot; geklickt werden und schon geht es los! . . Jetzt kann auf &quot;Open&quot; geklickt werden und schon startet sich ein Jupyter Notebook. Das Ganze sollte dann in etwa so aussehen (das README.md stammt schon aus dem verlinkten github-Repo): . . Wie erwähnt ist mein github Repo schon mit der Instanz verbunden. So sieht bislang mein neu angelegtes Repo in github aus: . . Dann lasst uns ein neues Jupyter Notebook anlegen. Einfach auf &quot;New&quot; -&gt; &quot;Python 3&quot; klicken, und es öffnet sich ein Notebook: . . Als nächstes will ich zeigen, wie sich mit Hilfe des Terminals ganz einfach neue packages installieren lassen (über pip), und wie sich das bestehende github Repo aus dem Terminal ansteuern lässt. Dafür muss man bei Jupyter einfach auf &quot;New&quot; und dann -&gt; &quot;Terminal&quot; klicken. . . Mit dem Befehl installieren wir aus fastai das fastbook. Dies funktioniert über pip install. . Jetzt will ich zeigen, wie sich das Terminal ganz einfach mit git verbinden lässt. Über git clone können wir öffentliche git-Repos direkt auf unsere Instanz klonen/kopieren: . . Dies finden wir auch direkt in Jupyter wieder: . . Jetzt wollen wir das Ganze in unser mit der Instanz verbundenem github Repo laden. Dies funktioniert über git add, git commit und git push im Terminal. . . Anschließend muss noch der User Name zum Repo angegeben werden und das dazugehörige Passwort. Das war&#39;s schon. Wie großartig ist das? So sieht das geupdatete github Repo aus: . . Als weitere großartige Möglichkeit bietet Paperspace in den extra dedizierten Ordnern &quot;datasets&quot; und &quot;storage&quot; an, eigene Dateien umsonst in Paperspace zu speichern. Das bedeutet, dass sich bei Neustarten der Instanzen oder neu anlegen der Instanzen diese Dateien immer da sind! Diese Funktionalität bietet Google Colab zum Beispiel nicht an. . Ich hoffe dieser Blog war hilfreich und ihr seid wenigstens halb so begeistert wie ich es bin. . Bleibt dran für die nächsten Posts! . Euer Lasse .",
            "url": "https://lschmiddey.github.io/fastpages_/2020/09/12/Setup-Paperspace.html",
            "relUrl": "/2020/09/12/Setup-Paperspace.html",
            "date": " • Sep 12, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Was ist Gradient Descent und wie funktioniert es?",
            "content": "Eine Coding Anleitung . (dieser Beitrag basiert lose auf dem von fastai zur Verfügung gestellten fastbook) . Dieser Blog Post ist nicht dafür gedacht aufzuzeigen, wie die mathematische Berechnung hinter Gradient Descent funktioniert. Denn zum Glück gibt es genau dafür Computer, die das Ganze wahrscheinlich millionenfach schneller berechnen können als wir. Ich will eine intuitive Erklärung für Gradient Descent liefern, wofür wir Gradient Descent überhaupt brauchen und wie wir mit simplen Python Code unseren eigenen auf Gradient Descent beruhenden Algorithmus bauen können. . Ich erinnere mich, dass ich zu Schulzeiten (und mehr als Abi-Mathe wird für diesen Post wahrhaftig nicht gebraucht) die Ableitungsregeln gelernt und auf Funktionen angewandt habe, aber wirklich Sinn und Zweck habe ich in dem Ganzen nie gesehen. Im besten Fall hat sich die Lehrerin einen an den Haaren herbeigezogenen Fall ausgedacht, um nicht einfach stumpf die Funktion zu liefern, die abgeleitet werden soll. Ich hoffe nach diesem Blog wird klarer, weswegen Ableitungen bzw. die dazugehörigen Gradienten doch eine ziemlich coole und nützliche Sache sind. . Zunächst einmal möchte ich zeigen, was Gradient Descent kann: . . Ein Algorithmus, der erkennen kann, was für ein Bär auf einem gegebenem Bild zu sehen ist? Was zur Hölle hat das mit Gradient Descent zu tun? Wenn du es bis zum Ende des Posts aushältst, wirst du das hoffentlich verstehen. . Gradient Descent hilft Computern dabei zu lernen. Das Ganze sollten wir uns vielleicht in einem Schaubild einmal näher anschauen: . . Die Idee ist dabei wie folgt: Wir haben ein Modell, welches anhand von Bildern erkennen soll, welche Art von Bär hier zu sehen ist. Nachdem wir genügend Bilder von verschiedenen Bären gesammelt haben, fangen wir an unser Modell zu fragen, was es glaubt für eine Art von Bären zu sehen (predict). Wir können uns nun Metriken überlegen, an denen wir erkennen können, wie gut diese Vorhersage unseres Modells ist, zum Beispiel wie häufig das Modell die richtige Bärenart vorhergesagt hat und wie häufig es falsch lag. Wir können daraus einen sogenannten Loss berechnen, was nichts anderes ist als das, was die Lehrer uns immer als Funktion beschrieben haben. Wir haben also eine Funktion die uns angibt, wie gut/schlecht unser Modell Bärenarten vorhersagen kann. Diese Funktion kann zum Beispiel eine simple quadratische Funktion sein, sie kann aber theoretisch jede nur erdenkliche Form annehmen. . Wir wollen unseren Loss minimieren, sprich unsere Funktion soll so gut wie es nur kann die Funktion lernen, wie es möglichst gut Bärenarten voneinander unterscheiden kann. Lass uns eine Sekunde darüber nachdenken. Wir benutzen eine Loss-Funktion, damit wir unserem Modell Feedback geben, wie gut/schlecht es die bisherige Aufgabe gelöst hat. Mithilfe dieser Loss-Funktion lernt unser Modell, die verschiedenen Bärenarten besser zu unterscheiden. Doch wie &quot;lernt&quot; das Modell? Hier kommt Gradient Descent ins Spiel. . Der Gradient, also die Ableitung, gibt uns an, um wie viel die Funktion größer wird, wenn wir (optisch gesprochen) einen kleinen Schritt nach rechts bzw. einen kleinen Schritt nach links gehen von dem Punkt, an dem wir uns gerade befinden. Dies ist die Steigung von dem Punkt, an welchem wir uns gerade befinden. Doch was bringt uns das? . Ich hoffe, du hast einen Moment darüber nachgedacht. Was wir wollen, ist möglichst gut die Bärenarten voneinander zu unterscheiden. Dies steuern wir über unsere Loss-Funktion. Und je geringer unsere Loss-Funktion, desto besser sind wir im Vorhersagen, was für eine Bärenart wir hier gerade haben. Durch den Gradienten wissen wir, in welche Richtung wir uns bewegen müssen, um unsere Loss-Funktion kleiner zu machen. . Noch mag das Ganze recht abstrakt klingen, schon in Kürze folgt hier das Beispiel in Python. Ich will das Ganze aber noch einmal zusammenfassen. Wir wollen etwas optimieren, zum Beispiel möglichst genau die Bärenart vorhersagen. Um diese Funktion zu optimieren, brauchen wir die Loss-Funktion, die uns angibt, wie gut/schlecht unser Modell/unsere Zielfunktion performt. Diese Funktionen können jegliche erdenkliche Formen annehmen. Dem Gradienten ist dies jedoch egal. Der Gradient kann uns zu jedem Ort, an welchem wir uns in der Funktion befinden sagen, was mit unser Loss-Funktion passiert, wenn wir uns ein kleines Stück in eine beliebeige Richtung bewegen. Dies nutzen wir als Feedback, um die Parameter der Zielfunktion so anzupassen, dass die Loss-Funktion kleiner wird, wir also besser die Bärenarten voneinander unterscheiden können. . Dies sind die Schritte, die in dem Schaubild erklärt sind: wir initialisieren die Werte unserer Zielfunktion (anfangs zufällig, weil wir nicht wissen, wie die richtige Funktion aussieht), wir lassen unser Modell Vorhersagen treffen, berechnen daraufhin den Loss und die dazugehörigen Gradienten, um dann durch die Gradienten die Parameter des Modells anzupassen. Diesen Prozess wiederholen wir solange, bis wir mit dem Endergebnis zufrieden sind .Dies sollte auf einem sogenannten Validierungs-Set festgelegt werden, also auf Bildern von Bären, die unser Modell im Trainingsloop nicht sieht. Vielleicht wäre es hier angebracht einmal darüber nachzudenken, warum wir nicht einfach den Trainingsloop solange wiederholen, bis wir alle Bärenarten durch Gradient Descent korrekt vorhersagen können (Stichwort: Overfitting). . Genug geredet, jetzt wollen wir das Ganze auch in Code sehen! . Code Beispiel . Wir wollen mit Hilfe des oben beschriebenen Prozesses eine Funktion finden, die möglichst genau den Verlauf folgender Funktion beschrieben kann: . import torch import matplotlib.pyplot as plt . x = torch.arange(-5,20).float(); x y = 0.75*(x-4)**2 + 0.5*x + 1 plt.scatter(x,y); . Was wir wollen ist die Parameter dieser Funktion zu schätzen. Vom ansehen der Daten können wir bereits auf die funktionale Form schließen - ein Polynom 3ten Grades. Was wir an diesem Beispiel schon erkennen können ist, dass die angenommene Zielfunktion eine große Rolle spielt. Hätten wir von den Daten auf eine quadratische Funktion geschlossen, würden wir die &quot;wahre&quot; Form der Funktion nie richtig bestimmen können. Deep Learning überkommt dieses &quot;Problem&quot;, indem es jede nur erdenkliche Funktion annehmen kann (dazu mehr in einem späteren Post). . def f(x, params): a,b,c,d = params return a*(x-b)**2 + (c*x) + d . Jetzt benötigen wir noch unsere Loss-Funktion (und ich hoffe hier wird klar, wie Loss-Funktion und Zielfunktion miteinander &quot;kommunizieren&quot;): . def mse(preds, targets): return ((preds-targets)**2).mean() . params = torch.randn(4).requires_grad_() params . tensor([ 1.1514, 0.2004, -0.8726, 0.5281], requires_grad=True) . Wir starten unsere Vorhersagen: . preds = f(x, params) plt.scatter(x,preds.detach().numpy()) . &lt;matplotlib.collections.PathCollection at 0x7fd49eb1e580&gt; . Wie gut sehen unsere Vorhersagen aus? . def show_preds(preds, ax=None): if ax is None: ax=plt.subplots()[1] ax.scatter(x, y) ax.scatter(x, preds.detach().numpy(), color=&#39;red&#39;) ax.set_ylim(-10,100) ax.set_xlim(-5,20) . show_preds(preds) . Wow! Ziemlich miserabel. Wie miserabel zeigt uns unser loss. . loss = mse(preds, y) loss . tensor(8606.2266, grad_fn=&lt;MeanBackward0&gt;) . Auf geht&#39;s Gradient! Zeig uns, wie wir unsere Parameter updaten müssen, damit der Loss kleiner wird. . loss.backward() params.grad . tensor([26820.2754, -4137.0151, 1819.5073, 114.5496]) . Dann lass uns unsere Paramter anpassen (wir multiplizieren den Gradienten mit der sogenannten Learning Rate, dazu in einem weiteren Blog Posts mehr). . lr = 1e-5 params.data -= lr * params.grad.data params.grad = None params . tensor([ 0.8832, 0.2418, -0.8908, 0.5269], requires_grad=True) . Ist unser Loss geringer geworden? . preds = f(x,params) mse(preds, y) . tensor(2857.6997, grad_fn=&lt;MeanBackward0&gt;) . show_preds(preds) . Zum Glück ja. . Wir wollen die steps wiederholen, sodass wir langsam und mithilfe von Gradient Descent unsere Zielfunktion finden. . def apply_step(params, prn=True): preds = f(x, params) loss = mse(preds, y) loss.backward() params.data -= lr * params.grad.data params.grad = None if prn: print(loss.item()) return preds lr=1e-5 for i in range(20): apply_step(params) . 204.3289794921875 201.9536590576172 201.0904083251953 200.72898864746094 200.5342254638672 200.39480590820312 200.27386474609375 200.1591033935547 200.0463104248047 199.9343719482422 199.82254028320312 199.71095275878906 199.59934997558594 199.4879150390625 199.3763885498047 199.26504516601562 199.1535186767578 199.04220581054688 198.93089294433594 198.8196258544922 . _,axs = plt.subplots(1,6,figsize=(24,3)) for ax in axs: show_preds(apply_step(params, False), ax) plt.tight_layout() . Wie man sieht, passt sich die rote Kurve, also unsere Vorhersagen, immer mehr der wahren Kurve an. Alles aufgrund von Gradient Descent! Und genau diese Technik und diese Schritte, die hier in diesem Blog Post aufgezeigt wurden, sind auch die Schritte, die dabei helfen, Neuronale Netze zu trainieren. Die dann wiederum Bären auseinander halten können. . Ich hoffe, durch diesen Post ist die Idee hinter Gradient Descent ein wenig greifbarer geworden und der Sinn und Zweck von Ableitungen könnte von einer anderen Seite vielleicht ein wenig verständlicher betrachtet werden. . Bleibt dran für die nächsten Posts! . Euer Lasse .",
            "url": "https://lschmiddey.github.io/fastpages_/2020/09/01/Gradient-Descent.html",
            "relUrl": "/2020/09/01/Gradient-Descent.html",
            "date": " • Sep 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Ich bin Lasse, ein Data Science Enthusiast! .",
          "url": "https://lschmiddey.github.io/fastpages_/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://lschmiddey.github.io/fastpages_/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}